{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9139239be4d3d26a",
   "metadata": {},
   "source": [
    "<img src=\"images/img.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a5ce1aa953b915",
   "metadata": {},
   "source": [
    "# CS5228 Project, Group 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "5287322f-3943-4058-8448-970302c9ff79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Auto reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f32f076115be79",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "In this part, we are going to perform some data preprocessing steps. This may include:\n",
    "* Data cleaning: handle missing values, duplicates, inconsistant or invalid vallues, outliers\n",
    "\n",
    "* Data reduction: reduce number of attributes, reduce number of attribute values\n",
    "\n",
    "* Data transformation: attribute construction, normalization\n",
    "\n",
    "* Data discretization: encode to numerical attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773fb74f",
   "metadata": {},
   "source": [
    "### Setting up the Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "5a8c6d3e9adfc751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "a084ae15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 25000 data points in training data, each with 30 attributes.\n"
     ]
    }
   ],
   "source": [
    "# Load file into pandas dataframe\n",
    "df = pd.read_csv('./data/train.csv')\n",
    "\n",
    "num_records, num_attributes = df.shape\n",
    "print(\"There are {} data points in training data, each with {} attributes.\". format(num_records, num_attributes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd41045",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389b04ed",
   "metadata": {},
   "source": [
    "Before data cleaning, remove the known attributes that are not meaningful to our prediction model:\n",
    "  * Meaningless idendifier: listing_id \n",
    "  * Attributes in free text: title, description, features, accessories\n",
    "  * Attribute with the same value: eco_category, indicative_price\n",
    "  * Attribute unlikely to affect price: curb_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "c02d46d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 25000 data points in training data, each with 18 attributes.\n"
     ]
    }
   ],
   "source": [
    "columns_to_drop = [\n",
    "    'listing_id',          # Meaningless identifier\n",
    "    'title',               # Attributes in free text\n",
    "    'description',\n",
    "    'features',\n",
    "    'accessories',\n",
    "    'eco_category',        # Attribute with the same value\n",
    "    'indicative_price',\n",
    "    # 'curb_weight',         # Attribute unlikely to affect price\n",
    "\n",
    "    'original_reg_date',\n",
    "    'lifespan',\n",
    "\n",
    "    # 'make',\n",
    "    # 'model',\n",
    "    # 'type_of_vehicle',\n",
    "    'transmission',\n",
    "    'fuel_type',\n",
    "    # 'no_of_owners',\n",
    "    'opc_scheme',\n",
    "    'lifespan',\n",
    "\n",
    "    # 'category',\n",
    "]\n",
    "\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "\n",
    "num_records, num_attributes = df.shape\n",
    "print(\"There are {} data points in training data, each with {} attributes.\". format(num_records, num_attributes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde0aaf8",
   "metadata": {},
   "source": [
    "### Print Missing Values\n",
    "Firstly, for each of the columns with missing value, check the number of rows with NaN values.\n",
    "There are 3 scenarios:\n",
    "1. NaN value is the major (e.g. fuel_type has 19121 rows with NaN values), we remove the corresponding attritubes.\n",
    "2. NaN value is the minor. We can choose to fill or delete related data points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "834807be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data\n",
      "Column 'make' has 1316 rows with NaN values.\n",
      "Column 'model' has 0 rows with NaN values.\n",
      "Column 'manufactured' has 7 rows with NaN values.\n",
      "Column 'reg_date' has 0 rows with NaN values.\n",
      "Column 'type_of_vehicle' has 0 rows with NaN values.\n",
      "Column 'category' has 0 rows with NaN values.\n",
      "Column 'curb_weight' has 307 rows with NaN values.\n",
      "Column 'power' has 2640 rows with NaN values.\n",
      "Column 'engine_cap' has 596 rows with NaN values.\n",
      "Column 'no_of_owners' has 18 rows with NaN values.\n",
      "Column 'depreciation' has 507 rows with NaN values.\n",
      "Column 'coe' has 0 rows with NaN values.\n",
      "Column 'road_tax' has 2632 rows with NaN values.\n",
      "Column 'dereg_value' has 220 rows with NaN values.\n",
      "Column 'mileage' has 5304 rows with NaN values.\n",
      "Column 'omv' has 64 rows with NaN values.\n",
      "Column 'arf' has 174 rows with NaN values.\n",
      "Column 'price' has 0 rows with NaN values.\n"
     ]
    }
   ],
   "source": [
    "# Calculate the number of NaN values in each specified column\n",
    "nan_counts = df.isna().sum()\n",
    "\n",
    "# Print the number of NaN values for each column\n",
    "print('Training data')\n",
    "for column, count in nan_counts.items():\n",
    "    print(f\"Column '{column}' has {count} rows with NaN values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eafaf03",
   "metadata": {},
   "source": [
    "### Remove Exact Duplicates\n",
    "We remove duplicated data points here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "8caa0e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 24993 data points in training data, each with 18 attributes.\n"
     ]
    }
   ],
   "source": [
    "df = df.drop_duplicates()\n",
    "\n",
    "num_records, num_attributes = df.shape\n",
    "print(\"There are {} data points in training data, each with {} attributes.\". format(num_records, num_attributes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a273d568",
   "metadata": {},
   "source": [
    "### Transform date time attributes to numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "563f78ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 24993 data points, each with 18 attributes.\n"
     ]
    }
   ],
   "source": [
    "df['reg_date'] = pd.to_datetime(df['reg_date'], format='%d-%b-%Y')\n",
    "df['reg_year'] = df['reg_date'].dt.year\n",
    "df = df.drop(columns=['reg_date'])\n",
    "\n",
    "num_records, num_attributes = df.shape\n",
    "print(\"There are {} data points, each with {} attributes.\". format(num_records, num_attributes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c25581",
   "metadata": {},
   "source": [
    "### Plot corellation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e9a132f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns_to_keep = [\n",
    "#     #'model',\n",
    "#     #'type_of_vehicle',\n",
    "\n",
    "#     'mileage',\n",
    "#     'manufactured',\n",
    "#     'reg_year',\n",
    "#     'dereg_value',\n",
    "#     'depreciation',\n",
    "#     'power',\n",
    "#     'coe',\n",
    "#     # 'arf',\n",
    "#     'omv',\n",
    "#     'price',\n",
    "#     # 'road_tax',\n",
    "#     'engine_cap',\n",
    "#     'curb_weight',\n",
    "\n",
    "#     # 'rare & exotic',\n",
    "#     # 'hybrid cars',\n",
    "\n",
    "#     # 'low mileage car',\n",
    "#     # 'almost new car',\n",
    "#     # 'parf car',\n",
    "\n",
    "#     # 'coe car',\n",
    "\n",
    "# ]\n",
    "\n",
    "# df = df[columns_to_keep]\n",
    "\n",
    "# correlation_matrix = df.corr()\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", square=True)\n",
    "# plt.title(\"Correlation Matrix of Attributes\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea8a0de",
   "metadata": {},
   "source": [
    "### Fill up other missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "dfc6a526",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\git\\CS5228_Project\\util\\DataPreprocess.py:73: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['manufactured'].fillna(df['reg_year'], inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values after handling:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\git\\CS5228_Project\\util\\DataPreprocess.py:144: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['mileage'].fillna((2024 - df['reg_year']) * 17500, inplace=True)\n",
      "e:\\git\\CS5228_Project\\util\\DataPreprocess.py:144: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['mileage'].fillna((2024 - df['reg_year']) * 17500, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "from util.DataPreprocess import HandlingMissingValues\n",
    "\n",
    "df = HandlingMissingValues(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28900a18-8bf3-4516-b663-c00533fde54f",
   "metadata": {},
   "source": [
    "### Transform categorical value to numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "829f7d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical_columns = [\n",
    "#     'make',\n",
    "#     'model',\n",
    "#     'type_of_vehicle',\n",
    "# ]\n",
    "\n",
    "# encode_dict = {}\n",
    "# le = LabelEncoder()\n",
    "# for column in categorical_columns:\n",
    "#     df[column] = le.fit_transform(df[column])\n",
    "#     encode_dict[column] = {str(label): int(index) for index, label in enumerate(le.classes_)}\n",
    "\n",
    "# with open('./data/encode.json', 'w') as file:\n",
    "#     json.dump(encode_dict, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a29cd1d",
   "metadata": {},
   "source": [
    "### Handle category attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "a43ae4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique categories: 15\n",
      "Unique categories: {'premium ad car', 'imported used vehicle', 'parf car', 'opc car', 'low mileage car', 'electric cars', 'sgcarmart warranty cars', 'almost new car', 'vintage cars', 'sta evaluated car', 'coe car', 'direct owner sale', 'hybrid cars', 'rare & exotic', 'consignment car'}\n",
      "There are 24259 data points, each with 32 attributes.\n"
     ]
    }
   ],
   "source": [
    "from util.DataPreprocess import HandlingCategoryAttribute\n",
    "\n",
    "df = HandlingCategoryAttribute(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a55c35-f9f7-42ed-a7aa-1e3edfffa7d4",
   "metadata": {},
   "source": [
    "### Remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c95ae45d-0846-4e1b-9f4d-2608349504b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from util.DataPreprocess import OutlierRemoval\n",
    "\n",
    "# df = OutlierRemoval(df, 'model', 'price')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11789885",
   "metadata": {},
   "source": [
    "### Saving the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "10b15971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame has been saved to 'data/xi/train_preprocessed.csv'.\n"
     ]
    }
   ],
   "source": [
    "file_name = 'data/xi/train_preprocessed.csv'\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(file_name):\n",
    "    # Delete the file\n",
    "    os.remove(file_name)\n",
    "    print(f\"Existing file '{file_name}' has been deleted.\")\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "df.to_csv(file_name, index=False)\n",
    "print(f\"DataFrame has been saved to '{file_name}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bc5a63",
   "metadata": {},
   "source": [
    "## Data Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a1b610-13c2-4709-9ad9-cb3482c3d209",
   "metadata": {},
   "source": [
    "### 1) Load preprocessed training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "e6e80ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 24259 data points in training data, each with 32 attributes.\n"
     ]
    }
   ],
   "source": [
    "# Load file into pandas dataframe, we saved our preprocessed file at path 'output_file'\n",
    "training_file = 'data/xi/train_preprocessed.csv'\n",
    "df = pd.read_csv(training_file)\n",
    "\n",
    "columns_to_keep = [\n",
    "    #'model',\n",
    "    #'type_of_vehicle',\n",
    "\n",
    "    'mileage',\n",
    "    'manufactured',\n",
    "    'reg_year',\n",
    "    'dereg_value',\n",
    "    'depreciation',\n",
    "    'power',\n",
    "    'coe',\n",
    "    # 'arf',\n",
    "    'omv',\n",
    "    'price',\n",
    "    # 'road_tax',\n",
    "    'engine_cap',\n",
    "    'curb_weight',\n",
    "\n",
    "    'rare & exotic',\n",
    "    # 'hybrid cars',\n",
    "\n",
    "    # 'low mileage car',\n",
    "    # 'almost new car',\n",
    "    # 'parf car',\n",
    "\n",
    "    # 'coe car',\n",
    "\n",
    "]\n",
    "\n",
    "df_new = df[columns_to_keep]\n",
    "\n",
    "num_records, num_attributes = df.shape\n",
    "print(\"There are {} data points in training data, each with {} attributes.\". format(num_records, num_attributes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5262d1b1",
   "metadata": {},
   "source": [
    "### Data Augmentation, copy rows with less than 5 samples by group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "7ec8c8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from util.DataPreprocess import DataAugmentation\n",
    "\n",
    "# df_aug = DataAugmentation(df)\n",
    "\n",
    "# num_records, num_attributes = df_aug.shape\n",
    "# print(\"There are {} data points after augmentation, each with {} attributes.\". format(num_records, num_attributes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1460dc",
   "metadata": {},
   "source": [
    "### 2) Split input attributes and output attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "ab5cd7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_new['price']\n",
    "X = df_new.drop(columns=['price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d9108c",
   "metadata": {},
   "source": [
    "### 3) Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "43ab598d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param = 1, RSME training = 36069.1 (587.2), RSME validation = 39168.8 (5227.9)\n",
      "param = 2, RSME training = 23935.2 (249.8), RSME validation = 29194.0 (3873.9)\n",
      "param = 3, RSME training = 15602.9 (319.7), RSME validation = 22345.7 (4131.3)\n",
      "param = 4, RSME training = 10612.6 (182.9), RSME validation = 19879.8 (5390.0)\n",
      "param = 5, RSME training = 7464.5 (74.1), RSME validation = 18988.9 (5892.2)\n",
      "param = 6, RSME training = 5473.4 (84.3), RSME validation = 19476.8 (6693.9)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "\n",
    "# Only considered hyperparameter: max depth of trees\n",
    "param_choices = [1, 2, 3, 4, 5, 6]\n",
    "# param_choices = [8, 10, 12, 15, 20]\n",
    "\n",
    "# Keep track of results for visualization\n",
    "param_to_scores = {}\n",
    "\n",
    "for param in param_choices:\n",
    "\n",
    "    # Train regressor with the current parameter setting\n",
    "    # regressor = DecisionTreeRegressor(max_depth=param)\n",
    "    # regressor = RandomForestRegressor(max_depth=param)\n",
    "    regressor = GradientBoostingRegressor(max_depth=param)\n",
    "    \n",
    "    # Perform 10-fold cross_validations\n",
    "    scores = cross_validate(regressor, X, y, cv=10, scoring='neg_root_mean_squared_error', return_train_score=True)\n",
    "    \n",
    "    # Extract the 10 RSME scores (training scores and validation scores) for each run/fold\n",
    "    # The (-1) is only needed since we get the negative root mean squared errors (it's a sklearn thing)\n",
    "    rsme_train = scores['train_score'] * (-1)\n",
    "    rsme_valid = scores['test_score'] * (-1)\n",
    "    \n",
    "    ## Keep track of all num_folds f1 scores for current param (for plotting)\n",
    "    param_to_scores[param] = (rsme_train, rsme_valid)\n",
    "    \n",
    "    ## Print statement for some immediate feedback (values in parenthesis represent the Standard Deviation)\n",
    "    print('param = {}, RSME training = {:.1f} ({:.1f}), RSME validation = {:.1f} ({:.1f})'\n",
    "          .format(param, np.mean(rsme_train), np.std(rsme_train), np.mean(rsme_valid), np.std(rsme_valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccdd425",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a874f15d",
   "metadata": {},
   "source": [
    "### 1) Load and preprocess test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "feae7cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\git\\CS5228_Project\\util\\DataPreprocess.py:178: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_test['manufactured'].fillna(df_test['reg_year'], inplace=True)\n",
      "e:\\git\\CS5228_Project\\util\\DataPreprocess.py:208: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_test['mileage'].fillna((2024 - df_test['reg_year']) * 17500, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values after handling:  50669\n",
      "Number of unique categories: 15\n",
      "Unique categories: {'premium ad car', 'imported used vehicle', 'parf car', 'opc car', 'electric cars', 'sta evaluated car', 'sgcarmart warranty cars', 'almost new car', 'vintage cars', 'consignment car', 'coe car', 'direct owner sale', 'hybrid cars', 'rare & exotic', 'low mileage car'}\n",
      "There are 10000 data points, each with 43 attributes.\n",
      "DataFrame has been saved to 'data/xi/test_preprocessed.csv'.\n"
     ]
    }
   ],
   "source": [
    "from util.DataPreprocess import HandlingMissingValuesTest\n",
    "\n",
    "test_file = './data/test.csv'\n",
    "df_test = pd.read_csv(test_file)\n",
    "\n",
    "df_test['reg_date'] = pd.to_datetime(df_test['reg_date'], format='%d-%b-%Y')\n",
    "df_test['reg_year'] = df_test['reg_date'].dt.year\n",
    "df_test = df_test.drop(columns=['reg_date'])\n",
    "\n",
    "df_test = HandlingMissingValuesTest(df, df_test)\n",
    "df_test = HandlingCategoryAttribute(df_test)\n",
    "\n",
    "file_name = 'data/xi/test_preprocessed.csv'\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(file_name):\n",
    "    # Delete the file\n",
    "    os.remove(file_name)\n",
    "    print(f\"Existing file '{file_name}' has been deleted.\")\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "df_test.to_csv(file_name, index=False)\n",
    "print(f\"DataFrame has been saved to '{file_name}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6da6ab4",
   "metadata": {},
   "source": [
    "### 2) Predict using GBR with selected hyperparameter from the previous fine-tuning step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "5b6d68f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data\n",
      "Column 'mileage' has 0 rows with NaN values.\n",
      "Column 'manufactured' has 0 rows with NaN values.\n",
      "Column 'reg_year' has 0 rows with NaN values.\n",
      "Column 'dereg_value' has 0 rows with NaN values.\n",
      "Column 'depreciation' has 0 rows with NaN values.\n",
      "Column 'power' has 0 rows with NaN values.\n",
      "Column 'coe' has 0 rows with NaN values.\n",
      "Column 'omv' has 0 rows with NaN values.\n",
      "Column 'engine_cap' has 0 rows with NaN values.\n",
      "Column 'curb_weight' has 0 rows with NaN values.\n",
      "Column 'rare & exotic' has 0 rows with NaN values.\n",
      "Prediction file 'predictions.csv' generated successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load file into pandas dataframe, we saved our preprocessed file at path 'output_file'\n",
    "test_file = 'data/xi/test_preprocessed.csv'\n",
    "df_test = pd.read_csv(test_file)\n",
    "\n",
    "columns_to_keep = [col for col in df_new.columns if col != 'price']\n",
    "\n",
    "df_test = df_test[columns_to_keep]\n",
    "df_test = df_test.fillna(X.mean())\n",
    "# Calculate the number of NaN values in each specified column\n",
    "nan_counts = df_test.isna().sum()\n",
    "\n",
    "# Print the number of NaN values for each column\n",
    "print('Test data')\n",
    "for column, count in nan_counts.items():\n",
    "    print(f\"Column '{column}' has {count} rows with NaN values.\")\n",
    "\n",
    "gbr_model = GradientBoostingRegressor(max_depth=4)\n",
    "gbr_model.fit(X, y)\n",
    "y_pred = gbr_model.predict(df_test)\n",
    "\n",
    "predictions_df = pd.DataFrame({\n",
    "    'Id': df_test.index,\n",
    "    'Predicted': y_pred\n",
    "})\n",
    "\n",
    "# Save to a CSV file\n",
    "predictions_df.to_csv('data/xi/predictions.csv', index=False)\n",
    "print(\"Prediction file 'predictions.csv' generated successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81365da5-d652-4c94-9cb9-0bdbc430dc9e",
   "metadata": {},
   "source": [
    "### Load test data and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51b11652-d408-4b2d-b7a0-a39b1dafd5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique categories: 15\n",
      "Unique categories: {'sgcarmart warranty cars', 'hybrid cars', 'rare & exotic', 'almost new car', 'direct owner sale', 'imported used vehicle', 'consignment car', 'vintage cars', 'electric cars', 'sta evaluated car', 'parf car', 'opc car', 'coe car', 'low mileage car', 'premium ad car'}\n",
      "There are 24258 data points, each with 17 attributes.\n"
     ]
    }
   ],
   "source": [
    "test_file = './data/test.csv'\n",
    "df_test = pd.read_csv(test_file)\n",
    "\n",
    "df_test['reg_date'] = pd.to_datetime(df_test['reg_date'], format='%d-%b-%Y')\n",
    "df_test['reg_year'] = df_test['reg_date'].dt.year\n",
    "df_test = df_test.drop(columns=['reg_date'])\n",
    "\n",
    "# Replace '-' with an empty string\n",
    "df_test['category'] = df_test['category'].replace('-', '')\n",
    "\n",
    "# Split the 'category' column into lists\n",
    "df_test['category_list'] = df_test['category'].str.split(', ')\n",
    "\n",
    "# Handle empty strings by replacing them with empty lists\n",
    "df_test['category_list'] = df_test['category_list'].apply(lambda x: [] if x == [''] else x)\n",
    "\n",
    "# Import itertools for flattening lists\n",
    "from itertools import chain\n",
    "\n",
    "# Flatten the list of lists to a single list\n",
    "all_categories = list(chain.from_iterable(df_test['category_list']))\n",
    "\n",
    "# Get the unique categories\n",
    "unique_categories = set(all_categories)\n",
    "\n",
    "# Print the number of unique categories\n",
    "print(f\"Number of unique categories: {len(unique_categories)}\")\n",
    "print(\"Unique categories:\", unique_categories)\n",
    "\n",
    "# Initialize the MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "# Fit and transform the category lists\n",
    "category_dummies = mlb.fit_transform(df_test['category_list'])\n",
    "\n",
    "# Create a DataFrame with the one-hot encoded categories\n",
    "category_df = pd.DataFrame(category_dummies, columns=mlb.classes_, index=df_test.index)\n",
    "\n",
    "# Concatenate the new dummy columns to the original DataFrame\n",
    "df_test = pd.concat([df_test, category_df], axis=1)\n",
    "\n",
    "# Drop the temporary 'category_list' column if desired\n",
    "df_test.drop('category_list', axis=1, inplace=True)\n",
    "df_test.drop('category', axis=1, inplace=True)\n",
    "\n",
    "num_records, num_attributes = df.shape\n",
    "\n",
    "print(\"There are {} data points, each with {} attributes.\". format(num_records, num_attributes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e85bd5-970c-425c-a0f0-dce631a8d3a7",
   "metadata": {},
   "source": [
    "### Select attributes on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e21a079-8608-48b9-bd9e-389eb321d103",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10000 data points, each with 43 attributes.\n",
      "There are 10000 data points in test data, each with 16 attributes.\n"
     ]
    }
   ],
   "source": [
    "num_records, num_attributes = df_test.shape\n",
    "print(\"There are {} data points, each with {} attributes.\". format(num_records, num_attributes))\n",
    "\n",
    "categorical_columns = [\n",
    "    'make',\n",
    "    'model',\n",
    "    'type_of_vehicle',\n",
    "    'transmission',\n",
    "]\n",
    "\n",
    "with open('./data/encode.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "for col, cate_dict in data.items():\n",
    "    if col in df_test.columns:\n",
    "        df_test[col] = df_test[col].map(cate_dict)\n",
    "\n",
    "df_test = df_test[columns_to_keep]\n",
    "\n",
    "num_records, num_attributes = df_test.shape\n",
    "print(\"There are {} data points in test data, each with {} attributes.\". format(num_records, num_attributes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd996336-d74b-4e21-b82a-dcd20d42cdcc",
   "metadata": {},
   "source": [
    "### Check if train data has all models in test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d57e5e92-f7c4-454d-8653-d515f7754e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df does not include {np.float64(nan)}\n"
     ]
    }
   ],
   "source": [
    "models_in_df = set(df['model'].unique())\n",
    "models_in_df_test = set(df_test['model'].unique())\n",
    "\n",
    "if models_in_df_test.issubset(models_in_df):\n",
    "    print(\"df includes all models in df_test\")\n",
    "else:\n",
    "    missing_models = models_in_df_test - models_in_df\n",
    "    print(\"df does not include\", missing_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c681594f-3de4-4511-8154-437e10dee1b6",
   "metadata": {},
   "source": [
    "### Mining code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4c22a91f-729c-4a22-a443-0f05af43f099",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.DataMining import split_dataframe, split_dataframe_flex\n",
    "from util.DataMining import (\n",
    "    RandomForestMining,\n",
    "    RandomForestMiningByModel,\n",
    "    GradientBoostingMining,\n",
    "    LinearRegressionMining,\n",
    "    LinearRegressionMiningByModel,\n",
    "    CombinedDataMiningRandomForestAndLinearRegression\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03355739-324a-4a13-a77a-bf301417c9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_times, rmse_sum = 5, 0\n",
    "# for i in tqdm(range(run_times), desc='Running Random Forest'):\n",
    "#     target_col = 'price'\n",
    "#     x_train, x_test, y_train, y_test = split_dataframe(df, target_col)\n",
    "#     rmse_sum += RandomForestMining(x_train, x_test, y_train, y_test)\n",
    "# print('Average RMSE:', round(rmse_sum / run_times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5469d35d-cf27-470e-97ab-9e4f0b67152d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Random Forest:   0%|          | 0/1 [00:00<?, ?it/s]e:\\git\\CS5228_Project\\util\\DataMining.py:120: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y_pred = pd.concat([y_pred, temp_df])\n",
      "Running Random Forest: 100%|██████████| 1/1 [03:29<00:00, 209.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to results.csv\n",
      "Running not in develop mode\n",
      "RMSE on test data: 15525.294495071965\n",
      "Average RMSE: 15525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_times, rmse_sum = 1, 0\n",
    "for i in tqdm(range(run_times), desc='Running Random Forest'):\n",
    "    train_drop_cols = ['price']\n",
    "    test_cols = ['price', 'model']\n",
    "    x_train, x_test, y_train, y_test = split_dataframe_flex(df_aug, train_drop_cols, test_cols)\n",
    "    rmse_sum += RandomForestMiningByModel(x_train, x_test, y_train, y_test)\n",
    "print('Average RMSE:', round(rmse_sum / run_times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f5716ffc-b01c-4851-b397-e498964f3afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train, y_train = df.drop(columns=['price']), df['price']\n",
    "# x_test = df_test[x_train.columns]\n",
    "\n",
    "# res = RandomForestMining(x_train, x_test, y_train, dev=True)\n",
    "# res.to_csv('./data/res.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4229aeee-e40d-4332-aab2-82beceb910a1",
   "metadata": {},
   "source": [
    "### This cell do prediction model by model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "900b06eb-489d-40c2-8722-61bcd5fd87a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/enqurance/Desktop/NUS/Sem1_AY2024-2025/CS5228/Projects/DataMining/util/DataMining.py:120: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y_pred = pd.concat([y_pred, temp_df])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     19560.955\n",
      "1     33476.140\n",
      "2    143657.020\n",
      "3     73018.820\n",
      "4     27141.215\n",
      "Name: Predicted, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = df.drop(columns=['price']), df[['price', 'model']]\n",
    "x_test = df_test[x_train.columns].dropna(subset=['model'])\n",
    "\n",
    "res_model = RandomForestMiningByModel(x_train, x_test, y_train, dev=True)\n",
    "print(res_model.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b1c5d2-e091-44e2-ab66-c4ef272ab0a6",
   "metadata": {},
   "source": [
    "### This cell do prediction on test data with 'model' attribute missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee95f3a0-b95a-463e-ad61-e8865d6f02d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/enqurance/anaconda3/envs/cs5228_project/lib/python3.10/site-packages/sklearn/base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Predicted\n",
      "21    56183.438972\n",
      "195  285485.266971\n",
      "212  166325.453607\n",
      "402   19893.851406\n",
      "412   73037.641540\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = df.drop(columns=['price', 'model']), df[['price']]\n",
    "x_test = df_test_unmapped[x_train.columns]\n",
    "\n",
    "res_nomodel = RandomForestMining(x_train, x_test, y_train, dev=True)\n",
    "print(res_nomodel.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "89a93b02-347e-45fb-82bd-b1627555e72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9902\n",
      "98\n"
     ]
    }
   ],
   "source": [
    "print(len(res_model))\n",
    "print(len(res_nomodel))\n",
    "res = pd.concat([res_model, res_nomodel])\n",
    "res.to_csv('./data/res_by_model_original.csv')\n",
    "res.reset_index(inplace=True)\n",
    "res.rename(columns={'index': 'Id'}, inplace=True)\n",
    "res_sorted = res.sort_values(by='Id')\n",
    "res_sorted.to_csv('./data/res_by_model2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387f2301-f281-4dfc-b417-c3966a2fe6aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
