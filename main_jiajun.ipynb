{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9139239be4d3d26a",
   "metadata": {},
   "source": [
    "<img src=\"images/img.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a5ce1aa953b915",
   "metadata": {},
   "source": [
    "# CS5228 Project, Group 32"
   ]
  },
  {
   "cell_type": "code",
   "id": "5287322f-3943-4058-8448-970302c9ff79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T16:18:03.194438Z",
     "start_time": "2024-10-29T16:18:02.976353Z"
    }
   },
   "source": [
    "# Auto reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "f7f32f076115be79",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "In this part, we are going to perform some data preprocessing steps. This may include:\n",
    "* Data cleaning: handle missing values, duplicates, inconsistant or invalid vallues, outliers\n",
    "\n",
    "* Data reduction: reduce number of attributes, reduce number of attribute values\n",
    "\n",
    "* Data transformation: attribute construction, normalization\n",
    "\n",
    "* Data discretization: encode to numerical attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773fb74f",
   "metadata": {},
   "source": [
    "### Setting up the Notebook"
   ]
  },
  {
   "cell_type": "code",
   "id": "5a8c6d3e9adfc751",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T16:18:03.640013Z",
     "start_time": "2024-10-29T16:18:03.198360Z"
    }
   },
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "a084ae15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T16:18:04.375103Z",
     "start_time": "2024-10-29T16:18:03.744041Z"
    }
   },
   "source": [
    "# Load file into pandas dataframe\n",
    "df = pd.read_csv('./data/train.csv')\n",
    "\n",
    "num_records, num_attributes = df.shape\n",
    "print(\"There are {} data points in training data, each with {} attributes.\". format(num_records, num_attributes))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 25000 data points in training data, each with 30 attributes.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "4fd41045",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389b04ed",
   "metadata": {},
   "source": [
    "Before data cleaning, remove the known attributes that are not meaningful to our prediction model:\n",
    "  * Meaningless idendifier: listing_id \n",
    "  * Attributes in free text: title, description, features, accessories\n",
    "  * Attribute with the same value: eco_category, indicative_price\n",
    "  * Attribute unlikely to affect price: curb_weight"
   ]
  },
  {
   "cell_type": "code",
   "id": "c02d46d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T16:18:05.442565Z",
     "start_time": "2024-10-29T16:18:05.253905Z"
    }
   },
   "source": [
    "columns_to_drop = [\n",
    "    'listing_id',          # Meaningless identifier\n",
    "    'title',               # Attributes in free text\n",
    "    'description',\n",
    "    'original_reg_date',\n",
    "    # 'curb_weight',         # Attribute unlikely to affect price\n",
    "    'lifespan',\n",
    "    'eco_category',        # Attribute with the same value\n",
    "    'features',\n",
    "    'accessories',\n",
    "    'indicative_price',\n",
    "]\n",
    "\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "\n",
    "num_records, num_attributes = df.shape\n",
    "print(\"There are {} data points in training data, each with {} attributes.\". format(num_records, num_attributes))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 25000 data points in training data, each with 21 attributes.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "dde0aaf8",
   "metadata": {},
   "source": [
    "### Handle Missing Values\n",
    "Firstly, for each of the columns with missing value, check the number of rows with NaN values.\n",
    "There are 3 scenarios:\n",
    "1. NaN value is the major (e.g. fuel_type has 19121 rows with NaN values), we remove the corresponding attritubes.\n",
    "2. NaN value is the minor. We can choose to fill or delete related data points. "
   ]
  },
  {
   "cell_type": "code",
   "id": "834807be",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-10-29T16:18:05.646901Z",
     "start_time": "2024-10-29T16:18:05.458270Z"
    }
   },
   "source": [
    "columns_to_check = [\n",
    "    'make',\n",
    "    'fuel_type',\n",
    "    'manufactured',\n",
    "    'power',\n",
    "    'engine_cap',\n",
    "    'mileage',\n",
    "    'no_of_owners',\n",
    "    'depreciation',\n",
    "    'road_tax',\n",
    "    'dereg_value',\n",
    "    'omv',\n",
    "    'arf',\n",
    "    'opc_scheme'\n",
    "]\n",
    "\n",
    "# Calculate the number of NaN values in each specified column\n",
    "nan_counts = df[columns_to_check].isna().sum()\n",
    "\n",
    "# Print the number of NaN values for each column\n",
    "print('Training data')\n",
    "for column, count in nan_counts.items():\n",
    "    print(f\"Column '{column}' has {count} rows with NaN values.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data\n",
      "Column 'make' has 1316 rows with NaN values.\n",
      "Column 'fuel_type' has 19121 rows with NaN values.\n",
      "Column 'manufactured' has 7 rows with NaN values.\n",
      "Column 'power' has 2640 rows with NaN values.\n",
      "Column 'engine_cap' has 596 rows with NaN values.\n",
      "Column 'mileage' has 5304 rows with NaN values.\n",
      "Column 'no_of_owners' has 18 rows with NaN values.\n",
      "Column 'depreciation' has 507 rows with NaN values.\n",
      "Column 'road_tax' has 2632 rows with NaN values.\n",
      "Column 'dereg_value' has 220 rows with NaN values.\n",
      "Column 'omv' has 64 rows with NaN values.\n",
      "Column 'arf' has 174 rows with NaN values.\n",
      "Column 'opc_scheme' has 24838 rows with NaN values.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "116b0e50",
   "metadata": {},
   "source": [
    "We delete attributes with TOO many NaN value here."
   ]
  },
  {
   "cell_type": "code",
   "id": "da6b4bc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T16:18:05.848985Z",
     "start_time": "2024-10-29T16:18:05.662982Z"
    }
   },
   "source": [
    "columns_to_drop_nan = [\n",
    "    'fuel_type',\n",
    "    'opc_scheme'\n",
    "]\n",
    "\n",
    "df = df.drop(columns=columns_to_drop_nan)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "7ea8a0de",
   "metadata": {},
   "source": [
    "Then we try to fill up other missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c55207db9bc68d",
   "metadata": {},
   "source": [
    "### Transform date time attributes to numerical values"
   ]
  },
  {
   "cell_type": "code",
   "id": "61435ab4cd070b75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T16:18:06.090845Z",
     "start_time": "2024-10-29T16:18:05.866804Z"
    }
   },
   "source": [
    "df['reg_date'] = pd.to_datetime(df['reg_date'], format='%d-%b-%Y')\n",
    "df['reg_year'] = df['reg_date'].dt.year\n",
    "df = df.drop(columns=['reg_date'])\n",
    "\n",
    "num_records, num_attributes = df.shape\n",
    "print(\"There are {} data points, each with {} attributes.\". format(num_records, num_attributes))\n",
    "# df.to_csv(\"./data/processing.csv\", index=False)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 25000 data points, each with 19 attributes.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Handling missing values in train",
   "id": "2ece0d8b692513f1"
  },
  {
   "cell_type": "code",
   "id": "dfc6a526",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T16:18:08.160646Z",
     "start_time": "2024-10-29T16:18:06.108472Z"
    }
   },
   "source": [
    "from util.DataPreprocess import HandlingMissingValues\n",
    "\n",
    "df = HandlingMissingValues(df)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values after handling:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\PythonWorkspace\\CS5228_Project\\util\\DataPreprocess.py:141: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, 'mileage'] = df['mileage'].fillna(mean_values)\n",
      "D:\\PythonWorkspace\\CS5228_Project\\util\\DataPreprocess.py:142: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, 'mileage'] = df['mileage'].round()\n",
      "D:\\PythonWorkspace\\CS5228_Project\\util\\DataPreprocess.py:144: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['mileage'].fillna((2024 - df['reg_year']) * 17500, inplace=True)\n",
      "D:\\PythonWorkspace\\CS5228_Project\\util\\DataPreprocess.py:148: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, 'omv'] = df['omv'].fillna(mean_values)\n",
      "D:\\PythonWorkspace\\CS5228_Project\\util\\DataPreprocess.py:149: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, 'omv'] = df['omv'].round()\n",
      "D:\\PythonWorkspace\\CS5228_Project\\util\\DataPreprocess.py:152: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, 'omv'] = df['omv'].fillna(mean_values)\n",
      "D:\\PythonWorkspace\\CS5228_Project\\util\\DataPreprocess.py:153: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, 'omv'] = df['omv'].round()\n",
      "D:\\PythonWorkspace\\CS5228_Project\\util\\DataPreprocess.py:157: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, 'curb_weight'] = df['curb_weight'].fillna(mean_values)\n",
      "D:\\PythonWorkspace\\CS5228_Project\\util\\DataPreprocess.py:158: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, 'curb_weight'] = df['curb_weight'].round()\n",
      "D:\\PythonWorkspace\\CS5228_Project\\util\\DataPreprocess.py:161: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, 'curb_weight'] = df['curb_weight'].fillna(mean_values)\n",
      "D:\\PythonWorkspace\\CS5228_Project\\util\\DataPreprocess.py:162: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, 'curb_weight'] = df['curb_weight'].round()\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "8eafaf03",
   "metadata": {},
   "source": [
    "### Remove Exact Duplicates\n",
    "We remove duplicated data points here."
   ]
  },
  {
   "cell_type": "code",
   "id": "8caa0e9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T16:18:08.423637Z",
     "start_time": "2024-10-29T16:18:08.216641Z"
    }
   },
   "source": [
    "df = df.drop_duplicates()\n",
    "\n",
    "num_records, num_attributes = df.shape\n",
    "print(\"There are {} data points in training data, each with {} attributes.\". format(num_records, num_attributes))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 24258 data points in training data, each with 19 attributes.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "84de8476-6545-4f89-ad57-b891986a9045",
   "metadata": {},
   "source": "### Merge rows with fewer data points on specific attributes(discarded idea)"
  },
  {
   "cell_type": "markdown",
   "id": "28900a18-8bf3-4516-b663-c00533fde54f",
   "metadata": {},
   "source": [
    "### Transform categorical value to numerical values"
   ]
  },
  {
   "cell_type": "code",
   "id": "829f7d64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T16:18:08.646403Z",
     "start_time": "2024-10-29T16:18:08.439272Z"
    }
   },
   "source": [
    "categorical_columns = [\n",
    "    'make',\n",
    "    'model',\n",
    "    'type_of_vehicle',\n",
    "    'transmission',\n",
    "]\n",
    "\n",
    "encode_dict = {}\n",
    "le = LabelEncoder()\n",
    "for column in categorical_columns:\n",
    "    df[column] = le.fit_transform(df[column])\n",
    "    encode_dict[column] = {str(label): int(index) for index, label in enumerate(le.classes_)}\n",
    "\n",
    "with open('./data/encode.json', 'w') as file:\n",
    "    json.dump(encode_dict, file, indent=4)"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "4a29cd1d",
   "metadata": {},
   "source": [
    "### Handle category attribute"
   ]
  },
  {
   "cell_type": "code",
   "id": "a43ae4a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T16:18:08.891174Z",
     "start_time": "2024-10-29T16:18:08.664055Z"
    }
   },
   "source": [
    "from util.DataPreprocess import HandlingCategoryAttribute\n",
    "\n",
    "df = HandlingCategoryAttribute(df)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique categories: 15\n",
      "Unique categories: {'low mileage car', 'consignment car', 'rare & exotic', 'hybrid cars', 'opc car', 'imported used vehicle', 'vintage cars', 'parf car', 'premium ad car', 'almost new car', 'direct owner sale', 'sta evaluated car', 'coe car', 'sgcarmart warranty cars', 'electric cars'}\n",
      "There are 24258 data points, each with 33 attributes.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "c0a55c35-f9f7-42ed-a7aa-1e3edfffa7d4",
   "metadata": {},
   "source": [
    "### Remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "id": "c95ae45d-0846-4e1b-9f4d-2608349504b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T16:18:09.099654Z",
     "start_time": "2024-10-29T16:18:08.908885Z"
    }
   },
   "source": [
    "# from util.DataPreprocess import OutlierRemoval\n",
    "\n",
    "# df = OutlierRemoval(df, 'model', 'price')"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "11789885",
   "metadata": {},
   "source": [
    "### Saving the Data"
   ]
  },
  {
   "cell_type": "code",
   "id": "10b15971",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T16:18:09.468740Z",
     "start_time": "2024-10-29T16:18:09.117108Z"
    }
   },
   "source": [
    "file_name = './data/train_preprocessed.csv'\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(file_name):\n",
    "    # Delete the file\n",
    "    os.remove(file_name)\n",
    "    print(f\"Existing file '{file_name}' has been deleted.\")\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "df.to_csv(file_name, index=False)\n",
    "print(f\"DataFrame has been saved to '{file_name}'.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing file './data/train_preprocessed.csv' has been deleted.\n",
      "DataFrame has been saved to './data/train_preprocessed.csv'.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "06bc5a63",
   "metadata": {},
   "source": [
    "## Data Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a1b610-13c2-4709-9ad9-cb3482c3d209",
   "metadata": {},
   "source": [
    "### Load preprocessed training data"
   ]
  },
  {
   "cell_type": "code",
   "id": "e6e80ea1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T16:18:09.712759Z",
     "start_time": "2024-10-29T16:18:09.484352Z"
    }
   },
   "source": [
    "# Load file into pandas dataframe, we saved our preprocessed file at path 'output_file'\n",
    "training_file = './data/train_preprocessed.csv'\n",
    "df = pd.read_csv(training_file)\n",
    "\n",
    "columns_to_keep = [\n",
    "    'model',\n",
    "    'mileage',\n",
    "    'low mileage car',\n",
    "    'manufactured',\n",
    "    'reg_year',\n",
    "    'type_of_vehicle',\n",
    "    'dereg_value',\n",
    "    'depreciation',\n",
    "    'power',\n",
    "    'coe',\n",
    "    'arf',\n",
    "    'omv',\n",
    "    'price',\n",
    "    'engine_cap',\n",
    "    'road_tax',\n",
    "    'almost new car',\n",
    "    'coe car',\n",
    "    'parf car',\n",
    "]\n",
    "\n",
    "df = df[columns_to_keep]\n",
    "columns_to_keep = [col for col in df.columns if col != 'price']\n",
    "\n",
    "num_records, num_attributes = df.shape\n",
    "print(\"There are {} data points in training data, each with {} attributes.\". format(num_records, num_attributes))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 24258 data points in training data, each with 18 attributes.\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "55faa8dc-6231-4b7d-b1de-bb0fc9405e03",
   "metadata": {},
   "source": [
    "### Data Augmentation, copy rows with less than 5 samples by group"
   ]
  },
  {
   "cell_type": "code",
   "id": "1ff32297-3b2c-4a56-94ae-4394247dcf4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T16:18:10.647015Z",
     "start_time": "2024-10-29T16:18:09.728384Z"
    }
   },
   "source": [
    "from util.DataPreprocess import DataAugmentation\n",
    "\n",
    "df_aug = DataAugmentation(df)\n",
    "\n",
    "num_records, num_attributes = df_aug.shape\n",
    "print(\"There are {} data points after augmentation, each with {} attributes.\". format(num_records, num_attributes))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 40290 data points after augmentation, each with 18 attributes.\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Train_preprocessed.csv",
   "id": "ec04fa4d7a7e5b44"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T16:18:11.056023Z",
     "start_time": "2024-10-29T16:18:10.666093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "file_name = './data/train_preprocessed.csv'\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(file_name):\n",
    "    # Delete the file\n",
    "    os.remove(file_name)\n",
    "    print(f\"Existing file '{file_name}' has been deleted.\")\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "df_aug.to_csv(file_name, index=False)\n",
    "print(f\"DataFrame has been saved to '{file_name}'.\")"
   ],
   "id": "f6b0b8d687b656d1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing file './data/train_preprocessed.csv' has been deleted.\n",
      "DataFrame has been saved to './data/train_preprocessed.csv'.\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load test data and preprocess reg_date and category",
   "id": "81365da5-d652-4c94-9cb9-0bdbc430dc9e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T16:18:11.362707Z",
     "start_time": "2024-10-29T16:18:11.071730Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_file = './data/test.csv'\n",
    "df_test = pd.read_csv(test_file)\n",
    "\n",
    "df_test['reg_date'] = pd.to_datetime(df_test['reg_date'], format='%d-%b-%Y')\n",
    "df_test['reg_year'] = df_test['reg_date'].dt.year\n",
    "df_test = df_test.drop(columns=['reg_date'])\n",
    "\n",
    "# Replace '-' with an empty string\n",
    "df_test['category'] = df_test['category'].replace('-', '')\n",
    "\n",
    "# Split the 'category' column into lists\n",
    "df_test['category_list'] = df_test['category'].str.split(', ')\n",
    "\n",
    "# Handle empty strings by replacing them with empty lists\n",
    "df_test['category_list'] = df_test['category_list'].apply(lambda x: [] if x == [''] else x)\n",
    "\n",
    "# Import itertools for flattening lists\n",
    "from itertools import chain\n",
    "\n",
    "# Flatten the list of lists to a single list\n",
    "all_categories = list(chain.from_iterable(df_test['category_list']))\n",
    "\n",
    "# Get the unique categories\n",
    "unique_categories = set(all_categories)\n",
    "\n",
    "# Print the number of unique categories\n",
    "print(f\"Number of unique categories: {len(unique_categories)}\")\n",
    "print(\"Unique categories:\", unique_categories)\n",
    "\n",
    "# Initialize the MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "# Fit and transform the category lists\n",
    "category_dummies = mlb.fit_transform(df_test['category_list'])\n",
    "\n",
    "# Create a DataFrame with the one-hot encoded categories\n",
    "category_df = pd.DataFrame(category_dummies, columns=mlb.classes_, index=df_test.index)\n",
    "\n",
    "# Concatenate the new dummy columns to the original DataFrame\n",
    "df_test = pd.concat([df_test, category_df], axis=1)\n",
    "\n",
    "# Drop the temporary 'category_list' column if desired\n",
    "df_test.drop('category_list', axis=1, inplace=True)\n",
    "df_test.drop('category', axis=1, inplace=True)\n",
    "\n",
    "num_records, num_attributes = df_test.shape\n",
    "\n",
    "print(\"There are {} data points, each with {} attributes.\". format(num_records, num_attributes))"
   ],
   "id": "51b11652-d408-4b2d-b7a0-a39b1dafd5ae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique categories: 15\n",
      "Unique categories: {'low mileage car', 'consignment car', 'rare & exotic', 'hybrid cars', 'opc car', 'imported used vehicle', 'vintage cars', 'parf car', 'premium ad car', 'almost new car', 'direct owner sale', 'sta evaluated car', 'coe car', 'sgcarmart warranty cars', 'electric cars'}\n",
      "There are 10000 data points, each with 43 attributes.\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "id": "f6e85bd5-970c-425c-a0f0-dce631a8d3a7",
   "metadata": {},
   "source": "### Select attributes on test data to keep the same attribute with train_preprocessed"
  },
  {
   "cell_type": "code",
   "id": "6e21a079-8608-48b9-bd9e-389eb321d103",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-10-29T16:18:11.580399Z",
     "start_time": "2024-10-29T16:18:11.378418Z"
    }
   },
   "source": [
    "num_records, num_attributes = df_test.shape\n",
    "print(\"There are {} data points, each with {} attributes.\". format(num_records, num_attributes))\n",
    "\n",
    "categorical_columns = [\n",
    "    'make',\n",
    "    'model',\n",
    "    'type_of_vehicle',\n",
    "    'transmission',\n",
    "]\n",
    "\n",
    "with open('./data/encode.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "for col, cate_dict in data.items():\n",
    "    if col in df_test.columns:\n",
    "        df_test[col] = df_test[col].map(cate_dict)\n",
    "\n",
    "df_test = df_test[columns_to_keep]\n",
    "\n",
    "num_records, num_attributes = df_test.shape\n",
    "print(\"There are {} data points in test data, each with {} attributes.\". format(num_records, num_attributes))\n",
    "# df_test.to_csv('./data/test_preprocessed.csv', index=False)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10000 data points, each with 43 attributes.\n",
      "There are 10000 data points in test data, each with 17 attributes.\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Before handling nan values in test_file",
   "id": "7fdb63a1a0c1af6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T16:18:11.840087Z",
     "start_time": "2024-10-29T16:18:11.599157Z"
    }
   },
   "cell_type": "code",
   "source": [
    "file_name = './data/test_preprocessed.csv'\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(file_name):\n",
    "    # Delete the file\n",
    "    os.remove(file_name)\n",
    "    print(f\"Existing file '{file_name}' has been deleted.\")\n",
    "\n",
    "# Save the Test est DataFrame to CSV\n",
    "df_test.to_csv(file_name, index=False)\n",
    "print(f\"Test dataFrame has been saved to '{file_name}'.\")"
   ],
   "id": "265668a2f435b6bf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing file './data/test_preprocessed.csv' has been deleted.\n",
      "Test dataFrame has been saved to './data/test_preprocessed.csv'.\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Handling nan values in test_file",
   "id": "ab81ab8cfd04e973"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T16:18:12.398709Z",
     "start_time": "2024-10-29T16:18:11.855638Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from util.DataPreprocess import HandlingMissingValuesTest2\n",
    "file_name = './data/test_preprocessed.csv'\n",
    "columns_to_keep = [\n",
    "    'model',\n",
    "    'mileage',\n",
    "    'low mileage car',\n",
    "    'manufactured',\n",
    "    'reg_year',\n",
    "    'type_of_vehicle',\n",
    "    'dereg_value',\n",
    "    'depreciation',\n",
    "    'power',\n",
    "    'coe',\n",
    "    'arf',\n",
    "    'omv',\n",
    "    'engine_cap',\n",
    "    'road_tax',\n",
    "    'almost new car',\n",
    "    'coe car',\n",
    "    'parf car',\n",
    "]\n",
    "\n",
    "# handling method 1\n",
    "df_test = HandlingMissingValuesTest2(df_aug, df_test)\n",
    "# Check for NaN values in the specified columns\n",
    "nan_columns = df_test[columns_to_keep].isnull().sum()\n",
    "nan_columns = nan_columns[nan_columns > 0]  # Keep only columns with NaN values\n",
    "\n",
    "print(\"Columns containing NaN values and their counts:\")\n",
    "print(nan_columns)\n"
   ],
   "id": "a4fef75b897b7dc5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values after handling:  98\n",
      "Columns containing NaN values and their counts:\n",
      "model    98\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "id": "cd996336-d74b-4e21-b82a-dcd20d42cdcc",
   "metadata": {},
   "source": [
    "### Check if train data has all models in test data"
   ]
  },
  {
   "cell_type": "code",
   "id": "d57e5e92-f7c4-454d-8653-d515f7754e86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T16:18:12.604736Z",
     "start_time": "2024-10-29T16:18:12.414420Z"
    }
   },
   "source": [
    "models_in_df = set(df['model'].unique())\n",
    "models_in_df_test = set(df_test['model'].unique())\n",
    "\n",
    "if models_in_df_test.issubset(models_in_df):\n",
    "    print(\"df includes all models in df_test\")\n",
    "else:\n",
    "    missing_models = models_in_df_test - models_in_df\n",
    "    print(\"df does not include\", missing_models)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df does not include {nan}\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "id": "c681594f-3de4-4511-8154-437e10dee1b6",
   "metadata": {},
   "source": [
    "### Mining code here"
   ]
  },
  {
   "cell_type": "code",
   "id": "4c22a91f-729c-4a22-a443-0f05af43f099",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T16:18:12.913972Z",
     "start_time": "2024-10-29T16:18:12.622452Z"
    }
   },
   "source": [
    "from util.DataMining import split_dataframe, split_dataframe_flex\n",
    "from util.DataMining import (\n",
    "    RandomForestMining,\n",
    "    RandomForestMiningByModel,\n",
    "    GradientBoostingMining,\n",
    "    LinearRegressionMining,\n",
    "    LinearRegressionMiningByModel,\n",
    "    CombinedDataMiningRandomForestAndLinearRegression\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "03355739-324a-4a13-a77a-bf301417c9ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T16:18:13.151023Z",
     "start_time": "2024-10-29T16:18:12.963502Z"
    }
   },
   "source": [
    "# run_times, rmse_sum = 5, 0\n",
    "# for i in tqdm(range(run_times), desc='Running Random Forest'):\n",
    "#     target_col = 'price'\n",
    "#     x_train, x_test, y_train, y_test = split_dataframe(df, target_col)\n",
    "#     rmse_sum += RandomForestMining(x_train, x_test, y_train, y_test)\n",
    "# print('Average RMSE:', round(rmse_sum / run_times))"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T16:35:12.197694Z",
     "start_time": "2024-10-29T16:26:05.829101Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from sklearn.model_selection import KFold\n",
    "# from tqdm import tqdm\n",
    "# \n",
    "# # 定义 K 值\n",
    "# k = 5\n",
    "# kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "# rmse_sum = 0\n",
    "# for i in tqdm(kf.split(df_aug), desc='Running Random Forest'):\n",
    "#     train_drop_cols = ['price']\n",
    "#     test_cols = ['price', 'model']\n",
    "#     x_train, x_test, y_train, y_test = split_dataframe_flex(df_aug, train_drop_cols, test_cols)\n",
    "#     rmse_sum += RandomForestMiningByModel(x_train, x_test, y_train, y_test)\n",
    "# print('Average RMSE:', round(rmse_sum / k))"
   ],
   "id": "b65693dac1fbceaf",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Random Forest: 1it [01:49, 109.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to results.csv\n",
      "Running not in develop mode\n",
      "RMSE on test data: 15245.877340628189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Random Forest: 2it [03:38, 109.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to results.csv\n",
      "Running not in develop mode\n",
      "RMSE on test data: 14990.719666651139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Random Forest: 3it [05:27, 109.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to results.csv\n",
      "Running not in develop mode\n",
      "RMSE on test data: 15058.312534627223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Random Forest: 4it [07:16, 109.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to results.csv\n",
      "Running not in develop mode\n",
      "RMSE on test data: 13102.0849458958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Random Forest: 5it [09:06, 109.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to results.csv\n",
      "Running not in develop mode\n",
      "RMSE on test data: 18484.529850129482\n",
      "Average RMSE: 15376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "id": "5469d35d-cf27-470e-97ab-9e4f0b67152d",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-10-29T16:20:17.509797Z",
     "start_time": "2024-10-29T16:18:13.168647Z"
    }
   },
   "source": [
    "run_times, rmse_sum = 1, 0\n",
    "for i in tqdm(range(run_times), desc='Running Random Forest'):\n",
    "    train_drop_cols = ['price']\n",
    "    test_cols = ['price', 'model']\n",
    "    x_train, x_test, y_train, y_test = split_dataframe_flex(df_aug, train_drop_cols, test_cols)\n",
    "    rmse_sum += RandomForestMiningByModel(x_train, x_test, y_train, y_test)\n",
    "print('Average RMSE:', round(rmse_sum / run_times))"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Random Forest: 100%|██████████| 1/1 [02:04<00:00, 124.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to results.csv\n",
      "Running not in develop mode\n",
      "RMSE on test data: 14583.364491397651\n",
      "Average RMSE: 14583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "f5716ffc-b01c-4851-b397-e498964f3afa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T16:20:17.804084Z",
     "start_time": "2024-10-29T16:20:17.603177Z"
    }
   },
   "source": [
    "# x_train, y_train = df.drop(columns=['price']), df['price']\n",
    "# x_test = df_test[x_train.columns]\n",
    "\n",
    "# res = RandomForestMining(x_train, x_test, y_train, dev=True)\n",
    "# res.to_csv('./data/res.csv', index=False)"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "id": "4229aeee-e40d-4332-aab2-82beceb910a1",
   "metadata": {},
   "source": [
    "### This cell do prediction model by model"
   ]
  },
  {
   "cell_type": "code",
   "id": "900b06eb-489d-40c2-8722-61bcd5fd87a6",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-10-29T16:23:01.100510Z",
     "start_time": "2024-10-29T16:20:17.804084Z"
    }
   },
   "source": [
    "x_train, y_train = df_aug.drop(columns=['price']), df_aug[['price', 'model']]\n",
    "x_test = df_test[x_train.columns].dropna(subset=['model'])\n",
    "\n",
    "res_model = RandomForestMiningByModel(x_train, x_test, y_train, dev=True)\n",
    "print(res_model.head())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     19581.940\n",
      "1     33418.030\n",
      "2    145927.600\n",
      "3     72150.145\n",
      "4     27236.805\n",
      "Name: Predicted, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "id": "79b1c5d2-e091-44e2-ab66-c4ef272ab0a6",
   "metadata": {},
   "source": [
    "### This cell do prediction on test data with 'model' attribute missing"
   ]
  },
  {
   "cell_type": "code",
   "id": "ee95f3a0-b95a-463e-ad61-e8865d6f02d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T16:39:27.903805Z",
     "start_time": "2024-10-29T16:38:01.347814Z"
    }
   },
   "source": [
    "x_train, y_train = df_aug.drop(columns=['price', 'model']), df_aug[['price']]\n",
    "df_test_unmapped = df_test[df_test['model'].isna()]\n",
    "x_test = df_test_unmapped[x_train.columns]\n",
    "\n",
    "res_nomodel = RandomForestMining(x_train, x_test, y_train, dev=True)\n",
    "print(res_nomodel.head())"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17768\\anaconda3\\lib\\site-packages\\sklearn\\base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Predicted\n",
      "21    59501.990428\n",
      "195  295143.191892\n",
      "212  179589.133772\n",
      "402   19316.375595\n",
      "412  105233.445044\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "id": "89a93b02-347e-45fb-82bd-b1627555e72f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T16:39:50.134389Z",
     "start_time": "2024-10-29T16:39:49.895381Z"
    }
   },
   "source": [
    "print(len(res_model))\n",
    "print(len(res_nomodel))\n",
    "res = pd.concat([res_model, res_nomodel])\n",
    "res.to_csv('./data/res_by_model_original.csv')\n",
    "res.reset_index(inplace=True)\n",
    "res.rename(columns={'index': 'Id'}, inplace=True)\n",
    "res_sorted = res.sort_values(by='Id')\n",
    "\n",
    "# Fill NaN values in the second column (index 1) with values from the third column (index 2)\n",
    "res_sorted.iloc[:, 1] = res_sorted.iloc[:, 1].fillna(res_sorted.iloc[:, 2])\n",
    "\n",
    "# Drop the third column (index 2)\n",
    "res_sorted = res_sorted.drop(res_sorted.columns[2], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# Modify the name of the second column (index 1)\n",
    "res_sorted.rename(columns={res_sorted.columns[1]: 'Predicted'}, inplace=True)\n",
    "res_sorted.to_csv('./data/res_by_model2.csv', index=False)\n",
    "# Display the result\n",
    "print(res_sorted.head())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9902\n",
      "98\n",
      "   Id   Predicted\n",
      "0   0   19581.940\n",
      "1   1   33418.030\n",
      "2   2  145927.600\n",
      "3   3   72150.145\n",
      "4   4   27236.805\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "id": "387f2301-f281-4dfc-b417-c3966a2fe6aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T16:23:01.419180Z",
     "start_time": "2024-10-29T16:16:26.463214Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
