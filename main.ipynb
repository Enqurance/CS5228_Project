{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9139239be4d3d26a",
   "metadata": {},
   "source": [
    "<img src=\"images/img.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a5ce1aa953b915",
   "metadata": {},
   "source": [
    "# CS5228 Project, Group 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5287322f-3943-4058-8448-970302c9ff79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a8c6d3e9adfc751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f32f076115be79",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "In this part, we are going to perform some data preprocessing steps. This may include:\n",
    "* Data cleaning: handle missing values, duplicates, inconsistant or invalid vallues, outliers\n",
    "\n",
    "* Data reduction: reduce number of attributes, reduce number of attribute values\n",
    "\n",
    "* Data transformation: attribute construction, normalization\n",
    "\n",
    "* Data discretization: encode to numerical attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773fb74f",
   "metadata": {},
   "source": [
    "### Load the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a084ae15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 25000 data points in training data, each with 30 attributes.\n"
     ]
    }
   ],
   "source": [
    "# Load file into pandas dataframe\n",
    "df = pd.read_csv('./data/train.csv')\n",
    "\n",
    "num_records, num_attributes = df.shape\n",
    "print(\"There are {} data points in training data, each with {} attributes.\". format(num_records, num_attributes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd41045",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389b04ed",
   "metadata": {},
   "source": [
    "Before data cleaning, remove the known attributes that are not meaningful to our prediction model:\n",
    "  * Meaningless idendifier: listing_id \n",
    "  * Attributes in free text: title, description, features, accessories\n",
    "  * Attribute with the same value: eco_category, indicative_price\n",
    "  * Attribute unlikely to affect price: curb_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c02d46d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 25000 data points in training data, each with 21 attributes.\n"
     ]
    }
   ],
   "source": [
    "columns_to_drop = [\n",
    "    'listing_id',          # Meaningless identifier\n",
    "    'title',               # Attributes in free text\n",
    "    'description',\n",
    "    'features',\n",
    "    'accessories',\n",
    "    'eco_category',        # Attribute with the same value\n",
    "    'indicative_price',\n",
    "    # 'curb_weight',         # Attribute unlikely to affect price\n",
    "    'original_reg_date',\n",
    "    'lifespan',\n",
    "]\n",
    "\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "\n",
    "num_records, num_attributes = df.shape\n",
    "print(\"There are {} data points in training data, each with {} attributes.\". format(num_records, num_attributes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde0aaf8",
   "metadata": {},
   "source": [
    "### Handle Missing Values\n",
    "Firstly, for each of the columns with missing value, check the number of rows with NaN values.\n",
    "There are 3 scenarios:\n",
    "1. NaN value is the major (e.g. fuel_type has 19121 rows with NaN values), we remove the corresponding attritubes.\n",
    "2. NaN value is the minor. We can choose to fill or delete related data points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "834807be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data\n",
      "Column 'make' has 1316 rows with NaN values.\n",
      "Column 'fuel_type' has 19121 rows with NaN values.\n",
      "Column 'manufactured' has 7 rows with NaN values.\n",
      "Column 'power' has 2640 rows with NaN values.\n",
      "Column 'engine_cap' has 596 rows with NaN values.\n",
      "Column 'mileage' has 5304 rows with NaN values.\n",
      "Column 'no_of_owners' has 18 rows with NaN values.\n",
      "Column 'depreciation' has 507 rows with NaN values.\n",
      "Column 'road_tax' has 2632 rows with NaN values.\n",
      "Column 'dereg_value' has 220 rows with NaN values.\n",
      "Column 'omv' has 64 rows with NaN values.\n",
      "Column 'arf' has 174 rows with NaN values.\n",
      "Column 'opc_scheme' has 24838 rows with NaN values.\n"
     ]
    }
   ],
   "source": [
    "columns_to_check = [\n",
    "    'make',\n",
    "    'fuel_type',\n",
    "    'manufactured',\n",
    "    'power',\n",
    "    'engine_cap',\n",
    "    'mileage',\n",
    "    'no_of_owners',\n",
    "    'depreciation',\n",
    "    'road_tax',\n",
    "    'dereg_value',\n",
    "    'omv',\n",
    "    'arf',\n",
    "    'opc_scheme'\n",
    "]\n",
    "\n",
    "# Calculate the number of NaN values in each specified column\n",
    "nan_counts = df[columns_to_check].isna().sum()\n",
    "\n",
    "# Print the number of NaN values for each column\n",
    "print('Training data')\n",
    "for column, count in nan_counts.items():\n",
    "    print(f\"Column '{column}' has {count} rows with NaN values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116b0e50",
   "metadata": {},
   "source": [
    "We delete attributes with TOO many NaN value here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da6b4bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop_nan = [\n",
    "    'fuel_type',\n",
    "    'opc_scheme'\n",
    "]\n",
    "\n",
    "df = df.drop(columns=columns_to_drop_nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09be75a",
   "metadata": {},
   "source": [
    "Transform date time attributes to numerical values\n",
    "This step is required to fill up the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce15c388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 25000 data points, each with 19 attributes.\n"
     ]
    }
   ],
   "source": [
    "df['reg_date'] = pd.to_datetime(df['reg_date'], format='%d-%b-%Y')\n",
    "df['reg_year'] = df['reg_date'].dt.year\n",
    "df = df.drop(columns=['reg_date'])\n",
    "\n",
    "num_records, num_attributes = df.shape\n",
    "print(\"There are {} data points, each with {} attributes.\". format(num_records, num_attributes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea8a0de",
   "metadata": {},
   "source": [
    "Then we try to fill up other missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfc6a526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values after handling:  0\n"
     ]
    }
   ],
   "source": [
    "from util.DataPreprocess import HandlingMissingValues\n",
    "\n",
    "df = HandlingMissingValues(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eafaf03",
   "metadata": {},
   "source": [
    "### Remove Exact Duplicates\n",
    "We remove duplicated data points here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8caa0e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 24258 data points in training data, each with 19 attributes.\n"
     ]
    }
   ],
   "source": [
    "df = df.drop_duplicates()\n",
    "\n",
    "num_records, num_attributes = df.shape\n",
    "print(\"There are {} data points in training data, each with {} attributes.\". format(num_records, num_attributes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28900a18-8bf3-4516-b663-c00533fde54f",
   "metadata": {},
   "source": [
    "### Transform categorical value to numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "829f7d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\n",
    "    'make',\n",
    "    'model',\n",
    "    'type_of_vehicle',\n",
    "    'transmission',\n",
    "]\n",
    "\n",
    "encode_dict = {}\n",
    "le = LabelEncoder()\n",
    "for column in categorical_columns:\n",
    "    df[column] = le.fit_transform(df[column])\n",
    "    encode_dict[column] = {str(label): int(index) for index, label in enumerate(le.classes_)}\n",
    "\n",
    "with open('./data/encode.json', 'w') as file:\n",
    "    json.dump(encode_dict, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a29cd1d",
   "metadata": {},
   "source": [
    "### Encode category attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a43ae4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique categories: 15\n",
      "Unique categories: {'imported used vehicle', 'sta evaluated car', 'vintage cars', 'almost new car', 'hybrid cars', 'rare & exotic', 'sgcarmart warranty cars', 'premium ad car', 'opc car', 'coe car', 'electric cars', 'low mileage car', 'direct owner sale', 'parf car', 'consignment car'}\n",
      "There are 24258 data points, each with 33 attributes.\n"
     ]
    }
   ],
   "source": [
    "from util.DataPreprocess import HandlingCategoryAttribute\n",
    "\n",
    "df = HandlingCategoryAttribute(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a55c35-f9f7-42ed-a7aa-1e3edfffa7d4",
   "metadata": {},
   "source": [
    "### Remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c95ae45d-0846-4e1b-9f4d-2608349504b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from util.DataPreprocess import OutlierRemoval\n",
    "\n",
    "# df = OutlierRemoval(df, 'model', 'price')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11789885",
   "metadata": {},
   "source": [
    "### Save the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10b15971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing file './data/train_preprocessed.csv' has been deleted.\n",
      "DataFrame has been saved to './data/train_preprocessed.csv'.\n"
     ]
    }
   ],
   "source": [
    "file_name = './data/train_preprocessed.csv'\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(file_name):\n",
    "    # Delete the file\n",
    "    os.remove(file_name)\n",
    "    print(f\"Existing file '{file_name}' has been deleted.\")\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "df.to_csv(file_name, index=False)\n",
    "print(f\"DataFrame has been saved to '{file_name}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bc5a63",
   "metadata": {},
   "source": [
    "## Data Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a1b610-13c2-4709-9ad9-cb3482c3d209",
   "metadata": {},
   "source": [
    "### Load preprocessed training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6e80ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 24258 data points in training data, each with 17 attributes.\n"
     ]
    }
   ],
   "source": [
    "# Load file into pandas dataframe, we saved our preprocessed file at path 'output_file'\n",
    "training_file = './data/train_preprocessed.csv'\n",
    "df = pd.read_csv(training_file)\n",
    "\n",
    "columns_to_keep = [\n",
    "    'model',\n",
    "    'mileage',\n",
    "    'low mileage car',\n",
    "    'manufactured',\n",
    "    'reg_year',\n",
    "    'type_of_vehicle',\n",
    "    'dereg_value',\n",
    "    'depreciation',\n",
    "    'power',\n",
    "    'coe',\n",
    "    'arf',\n",
    "    'omv',\n",
    "    'price',\n",
    "    'road_tax',\n",
    "    'almost new car',\n",
    "    'coe car',\n",
    "    'parf car',\n",
    "]\n",
    "\n",
    "df = df[columns_to_keep]\n",
    "columns_to_keep = [col for col in df.columns if col != 'price']\n",
    "\n",
    "num_records, num_attributes = df.shape\n",
    "print(\"There are {} data points in training data, each with {} attributes.\". format(num_records, num_attributes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77da5708",
   "metadata": {},
   "source": [
    "### Data Augmentation, copy rows with less than 5 samples by group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1aa49ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 40290 data points after augmentation, each with 17 attributes.\n"
     ]
    }
   ],
   "source": [
    "from util.DataPreprocess import DataAugmentation\n",
    "\n",
    "df_aug = DataAugmentation(df)\n",
    "\n",
    "num_records, num_attributes = df_aug.shape\n",
    "print(\"There are {} data points after augmentation, each with {} attributes.\". format(num_records, num_attributes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca582db1",
   "metadata": {},
   "source": [
    "### Save the augmentation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4d43381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing file './data/train_preprocessed_augmentation.csv' has been deleted.\n",
      "DataFrame has been saved to './data/train_preprocessed_augmentation.csv'.\n"
     ]
    }
   ],
   "source": [
    "file_name = './data/train_preprocessed_augmentation.csv'\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(file_name):\n",
    "    # Delete the file\n",
    "    os.remove(file_name)\n",
    "    print(f\"Existing file '{file_name}' has been deleted.\")\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "df_aug.to_csv(file_name, index=False)\n",
    "print(f\"DataFrame has been saved to '{file_name}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81365da5-d652-4c94-9cb9-0bdbc430dc9e",
   "metadata": {},
   "source": [
    "### Load test data and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51b11652-d408-4b2d-b7a0-a39b1dafd5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique categories: 15\n",
      "Unique categories: {'imported used vehicle', 'sta evaluated car', 'vintage cars', 'almost new car', 'hybrid cars', 'sgcarmart warranty cars', 'rare & exotic', 'premium ad car', 'opc car', 'coe car', 'electric cars', 'low mileage car', 'direct owner sale', 'parf car', 'consignment car'}\n",
      "There are 24258 data points, each with 17 attributes.\n"
     ]
    }
   ],
   "source": [
    "test_file = './data/test.csv'\n",
    "df_test = pd.read_csv(test_file)\n",
    "\n",
    "df_test['reg_date'] = pd.to_datetime(df_test['reg_date'], format='%d-%b-%Y')\n",
    "df_test['reg_year'] = df_test['reg_date'].dt.year\n",
    "df_test = df_test.drop(columns=['reg_date'])\n",
    "\n",
    "# Replace '-' with an empty string\n",
    "df_test['category'] = df_test['category'].replace('-', '')\n",
    "\n",
    "# Split the 'category' column into lists\n",
    "df_test['category_list'] = df_test['category'].str.split(', ')\n",
    "\n",
    "# Handle empty strings by replacing them with empty lists\n",
    "df_test['category_list'] = df_test['category_list'].apply(lambda x: [] if x == [''] else x)\n",
    "\n",
    "# Import itertools for flattening lists\n",
    "from itertools import chain\n",
    "\n",
    "# Flatten the list of lists to a single list\n",
    "all_categories = list(chain.from_iterable(df_test['category_list']))\n",
    "\n",
    "# Get the unique categories\n",
    "unique_categories = set(all_categories)\n",
    "\n",
    "# Print the number of unique categories\n",
    "print(f\"Number of unique categories: {len(unique_categories)}\")\n",
    "print(\"Unique categories:\", unique_categories)\n",
    "\n",
    "# Initialize the MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "# Fit and transform the category lists\n",
    "category_dummies = mlb.fit_transform(df_test['category_list'])\n",
    "\n",
    "# Create a DataFrame with the one-hot encoded categories\n",
    "category_df = pd.DataFrame(category_dummies, columns=mlb.classes_, index=df_test.index)\n",
    "\n",
    "# Concatenate the new dummy columns to the original DataFrame\n",
    "df_test = pd.concat([df_test, category_df], axis=1)\n",
    "\n",
    "# Drop the temporary 'category_list' column if desired\n",
    "df_test.drop('category_list', axis=1, inplace=True)\n",
    "df_test.drop('category', axis=1, inplace=True)\n",
    "\n",
    "num_records, num_attributes = df.shape\n",
    "\n",
    "print(\"There are {} data points, each with {} attributes.\". format(num_records, num_attributes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e85bd5-970c-425c-a0f0-dce631a8d3a7",
   "metadata": {},
   "source": [
    "### Select attributes on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e21a079-8608-48b9-bd9e-389eb321d103",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10000 data points, each with 43 attributes.\n",
      "There are 10000 data points in test data, each with 16 attributes.\n"
     ]
    }
   ],
   "source": [
    "num_records, num_attributes = df_test.shape\n",
    "print(\"There are {} data points, each with {} attributes.\". format(num_records, num_attributes))\n",
    "\n",
    "categorical_columns = [\n",
    "    'make',\n",
    "    'model',\n",
    "    'type_of_vehicle',\n",
    "    'transmission',\n",
    "]\n",
    "\n",
    "with open('./data/encode.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "for col, cate_dict in data.items():\n",
    "    if col in df_test.columns:\n",
    "        df_test[col] = df_test[col].map(cate_dict)\n",
    "\n",
    "df_test = df_test[columns_to_keep]\n",
    "\n",
    "num_records, num_attributes = df_test.shape\n",
    "print(\"There are {} data points in test data, each with {} attributes.\". format(num_records, num_attributes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd996336-d74b-4e21-b82a-dcd20d42cdcc",
   "metadata": {},
   "source": [
    "### Check if train data has all models in test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d57e5e92-f7c4-454d-8653-d515f7754e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df does not include {nan}\n"
     ]
    }
   ],
   "source": [
    "models_in_df = set(df['model'].unique())\n",
    "models_in_df_test = set(df_test['model'].unique())\n",
    "\n",
    "if models_in_df_test.issubset(models_in_df):\n",
    "    print(\"df includes all models in df_test\")\n",
    "else:\n",
    "    missing_models = models_in_df_test - models_in_df\n",
    "    print(\"df does not include\", missing_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c681594f-3de4-4511-8154-437e10dee1b6",
   "metadata": {},
   "source": [
    "### Mining code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c22a91f-729c-4a22-a443-0f05af43f099",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.DataMining import split_dataframe, split_dataframe_flex\n",
    "from util.DataMining import (\n",
    "    RandomForestMining,\n",
    "    RandomForestMiningByModel,\n",
    "    GradientBoostingMining,\n",
    "    LinearRegressionMining,\n",
    "    LinearRegressionMiningByModel,\n",
    "    CombinedDataMiningRandomForestAndLinearRegression\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03355739-324a-4a13-a77a-bf301417c9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_times, rmse_sum = 5, 0\n",
    "# for i in tqdm(range(run_times), desc='Running Random Forest'):\n",
    "#     target_col = 'price'\n",
    "#     x_train, x_test, y_train, y_test = split_dataframe(df, target_col)\n",
    "#     rmse_sum += RandomForestMining(x_train, x_test, y_train, y_test)\n",
    "# print('Average RMSE:', round(rmse_sum / run_times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5469d35d-cf27-470e-97ab-9e4f0b67152d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Random Forest:   0%|                                                                                                            | 0/1 [00:00<?, ?it/s]/Users/enqurance/Desktop/NUS/Sem1_AY2024-2025/CS5228/Projects/DataMining/util/DataMining.py:120: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y_pred = pd.concat([y_pred, temp_df])\n",
      "Running Random Forest: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [01:24<00:00, 84.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to results.csv\n",
      "Running not in develop mode\n",
      "RMSE on test data: 12594.510893969247\n",
      "Average RMSE: 12595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_times, rmse_sum = 1, 0\n",
    "for i in tqdm(range(run_times), desc='Running Random Forest'):\n",
    "    train_drop_cols = ['price']\n",
    "    test_cols = ['price', 'model']\n",
    "    x_train, x_test, y_train, y_test = split_dataframe_flex(df_aug, train_drop_cols, test_cols)\n",
    "    rmse_sum += RandomForestMiningByModel(x_train, x_test, y_train, y_test)\n",
    "print('Average RMSE:', round(rmse_sum / run_times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f5716ffc-b01c-4851-b397-e498964f3afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train, y_train = df.drop(columns=['price']), df['price']\n",
    "# x_test = df_test[x_train.columns]\n",
    "\n",
    "# res = RandomForestMining(x_train, x_test, y_train, dev=True)\n",
    "# res.to_csv('./data/res.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4229aeee-e40d-4332-aab2-82beceb910a1",
   "metadata": {},
   "source": [
    "### This cell do prediction model by model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "900b06eb-489d-40c2-8722-61bcd5fd87a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/enqurance/Desktop/NUS/Sem1_AY2024-2025/CS5228/Projects/DataMining/util/DataMining.py:120: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y_pred = pd.concat([y_pred, temp_df])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     19560.955\n",
      "1     33476.140\n",
      "2    143657.020\n",
      "3     73018.820\n",
      "4     27141.215\n",
      "Name: Predicted, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = df.drop(columns=['price']), df[['price', 'model']]\n",
    "x_test = df_test[x_train.columns].dropna(subset=['model'])\n",
    "\n",
    "res_model = RandomForestMiningByModel(x_train, x_test, y_train, dev=True)\n",
    "print(res_model.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b1c5d2-e091-44e2-ab66-c4ef272ab0a6",
   "metadata": {},
   "source": [
    "### This cell do prediction on test data with 'model' attribute missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee95f3a0-b95a-463e-ad61-e8865d6f02d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/enqurance/anaconda3/envs/cs5228_project/lib/python3.10/site-packages/sklearn/base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Predicted\n",
      "21    56183.438972\n",
      "195  285485.266971\n",
      "212  166325.453607\n",
      "402   19893.851406\n",
      "412   73037.641540\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = df.drop(columns=['price', 'model']), df[['price']]\n",
    "df_test_unmapped = df_test[df_test['model'].isna()]\n",
    "x_test = df_test_unmapped[x_train.columns]\n",
    "\n",
    "res_nomodel = RandomForestMining(x_train, x_test, y_train, dev=True)\n",
    "print(res_nomodel.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "89a93b02-347e-45fb-82bd-b1627555e72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9902\n",
      "98\n"
     ]
    }
   ],
   "source": [
    "print(len(res_model))\n",
    "print(len(res_nomodel))\n",
    "res = pd.concat([res_model, res_nomodel])\n",
    "res.to_csv('./data/res_by_model_original.csv')\n",
    "res.reset_index(inplace=True)\n",
    "res.rename(columns={'index': 'Id'}, inplace=True)\n",
    "res_sorted = res.sort_values(by='Id')\n",
    "res_sorted.to_csv('./data/res_by_model2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387f2301-f281-4dfc-b417-c3966a2fe6aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
