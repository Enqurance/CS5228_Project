{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9139239be4d3d26a",
   "metadata": {},
   "source": [
    "<img src=\"images/img.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a5ce1aa953b915",
   "metadata": {},
   "source": [
    "# CS5228 Project, Group 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f32f076115be79",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "In this part, we are going to perform some data preprocessing steps. This may include:\n",
    "* Data cleaning: handle missing values, duplicates, inconsistant or invalid vallues, outliers\n",
    "\n",
    "* Data reduction: reduce number of attributes, reduce number of attribute values\n",
    "\n",
    "* Data transformation: attribute construction, normalization\n",
    "\n",
    "* Data discretization: encode to numerical attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773fb74f",
   "metadata": {},
   "source": [
    "### Setting up the Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a8c6d3e9adfc751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a084ae15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 25000 data points in training data, each with 30 attributes.\n"
     ]
    }
   ],
   "source": [
    "# Load file into pandas dataframe\n",
    "df = pd.read_csv('./data/train.csv')\n",
    "\n",
    "num_records, num_attributes = df.shape\n",
    "print(\"There are {} data points in training data, each with {} attributes.\". format(num_records, num_attributes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd41045",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389b04ed",
   "metadata": {},
   "source": [
    "Before data cleaning, remove the known attributes that are not meaningful to our prediction model:\n",
    "  * Meaningless idendifier: listing_id \n",
    "  * Attributes in free text: title, description, features, accessories\n",
    "  * Attribute with the same value: eco_category, indicative_price\n",
    "  * Attribute unlikely to affect price: curb_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c02d46d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 25000 data points, each with 20 attributes.\n"
     ]
    }
   ],
   "source": [
    "columns_to_drop = [\n",
    "    'listing_id',          # Meaningless identifier\n",
    "    'title',               # Attributes in free text\n",
    "    'description',\n",
    "    'features',\n",
    "    'accessories',\n",
    "    'eco_category',        # Attribute with the same value\n",
    "    'indicative_price',\n",
    "    'curb_weight',         # Attribute unlikely to affect price\n",
    "    'original_reg_date',\n",
    "    'lifespan',\n",
    "]\n",
    "\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "\n",
    "num_records, num_attributes = df.shape\n",
    "print(\"There are {} data points, each with {} attributes.\". format(num_records, num_attributes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde0aaf8",
   "metadata": {},
   "source": [
    "### Handle Missing Values\n",
    "Firstly, for each of the columns with missing value, check the number of rows with NaN values.\n",
    "There are 3 scenarios:\n",
    "1. NaN value is the major (e.g. fuel_type has 19121 rows with NaN values), we remove the corresponding attritubes.\n",
    "2. NaN value is the minor. We can choose to fill or delete related data points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "834807be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Training data-----\n",
      "Column 'make' has 1316 rows with NaN values.\n",
      "Column 'fuel_type' has 19121 rows with NaN values.\n",
      "Column 'manufactured' has 7 rows with NaN values.\n",
      "Column 'power' has 2640 rows with NaN values.\n",
      "Column 'engine_cap' has 596 rows with NaN values.\n",
      "Column 'mileage' has 5304 rows with NaN values.\n",
      "Column 'no_of_owners' has 18 rows with NaN values.\n",
      "Column 'depreciation' has 507 rows with NaN values.\n",
      "Column 'road_tax' has 2632 rows with NaN values.\n",
      "Column 'dereg_value' has 220 rows with NaN values.\n",
      "Column 'omv' has 64 rows with NaN values.\n",
      "Column 'arf' has 174 rows with NaN values.\n",
      "Column 'opc_scheme' has 24838 rows with NaN values.\n"
     ]
    }
   ],
   "source": [
    "columns_to_check = [\n",
    "    'make',\n",
    "    'fuel_type',\n",
    "    'manufactured',\n",
    "    'power',\n",
    "    'engine_cap',\n",
    "    'mileage',\n",
    "    'no_of_owners',\n",
    "    'depreciation',\n",
    "    'road_tax',\n",
    "    'dereg_value',\n",
    "    'omv',\n",
    "    'arf',\n",
    "    'opc_scheme'\n",
    "]\n",
    "\n",
    "# Calculate the number of NaN values in each specified column\n",
    "nan_counts = df[columns_to_check].isna().sum()\n",
    "\n",
    "# Print the number of NaN values for each column\n",
    "print('-----Training data-----')\n",
    "for column, count in nan_counts.items():\n",
    "    print(f\"Column '{column}' has {count} rows with NaN values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116b0e50",
   "metadata": {},
   "source": [
    "We delete attributes with TOO many NaN value here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da6b4bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop_nan = [\n",
    "    'fuel_type',\n",
    "    'opc_scheme'\n",
    "]\n",
    "\n",
    "df = df.drop(columns=columns_to_drop_nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea8a0de",
   "metadata": {},
   "source": [
    "Then we try to fill up other missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfc6a526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values after handling:  0\n"
     ]
    }
   ],
   "source": [
    "from util.DataPreprocess import HandlingMissingValues\n",
    "\n",
    "df = HandlingMissingValues(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eafaf03",
   "metadata": {},
   "source": [
    "### Remove Exact Duplicates\n",
    "We remove duplicated data points here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8caa0e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 24258 data points, each with 18 attributes.\n"
     ]
    }
   ],
   "source": [
    "df = df.drop_duplicates()\n",
    "\n",
    "num_records, num_attributes = df.shape\n",
    "print(\"There are {} data points, each with {} attributes.\". format(num_records, num_attributes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84de8476-6545-4f89-ad57-b891986a9045",
   "metadata": {},
   "source": [
    "### Merge rows with fewer data points on specific attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3a7ec8a-8007-4aec-b501-2b4bfb4192ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 2\n",
    "\n",
    "value_counts = df['make'].value_counts()\n",
    "categories_to_replace = value_counts[value_counts < threshold].index\n",
    "df['make'] = df['make'].replace(categories_to_replace, 'others')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28900a18-8bf3-4516-b663-c00533fde54f",
   "metadata": {},
   "source": [
    "### Transform categorical value to numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "829f7d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\n",
    "    'make',\n",
    "    'model',\n",
    "    'type_of_vehicle',\n",
    "    'transmission',\n",
    "]\n",
    "\n",
    "le = LabelEncoder()\n",
    "for column in categorical_columns:\n",
    "    df[column] = le.fit_transform(df[column])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a273d568",
   "metadata": {},
   "source": [
    "### Transform date time attributes to numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f580a7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 24258 data points, each with 18 attributes.\n"
     ]
    }
   ],
   "source": [
    "df['reg_date'] = pd.to_datetime(df['reg_date'], format='%d-%b-%Y')\n",
    "df['reg_year'] = df['reg_date'].dt.year\n",
    "df = df.drop(columns=['reg_date'])\n",
    "\n",
    "num_records, num_attributes = df.shape\n",
    "print(\"There are {} data points, each with {} attributes.\". format(num_records, num_attributes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a29cd1d",
   "metadata": {},
   "source": [
    "### Handle category attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a43ae4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique categories: 15\n",
      "Unique categories: {'low mileage car', 'sgcarmart warranty cars', 'direct owner sale', 'opc car', 'imported used vehicle', 'sta evaluated car', 'premium ad car', 'almost new car', 'vintage cars', 'hybrid cars', 'parf car', 'electric cars', 'consignment car', 'coe car', 'rare & exotic'}\n",
      "There are 24258 data points, each with 32 attributes.\n"
     ]
    }
   ],
   "source": [
    "# Replace '-' with an empty string\n",
    "df['category'] = df['category'].replace('-', '')\n",
    "\n",
    "# Split the 'category' column into lists\n",
    "df['category_list'] = df['category'].str.split(', ')\n",
    "\n",
    "# Handle empty strings by replacing them with empty lists\n",
    "df['category_list'] = df['category_list'].apply(lambda x: [] if x == [''] else x)\n",
    "\n",
    "# Import itertools for flattening lists\n",
    "from itertools import chain\n",
    "\n",
    "# Flatten the list of lists to a single list\n",
    "all_categories = list(chain.from_iterable(df['category_list']))\n",
    "\n",
    "# Get the unique categories\n",
    "unique_categories = set(all_categories)\n",
    "\n",
    "# Print the number of unique categories\n",
    "print(f\"Number of unique categories: {len(unique_categories)}\")\n",
    "print(\"Unique categories:\", unique_categories)\n",
    "\n",
    "# Initialize the MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "# Fit and transform the category lists\n",
    "category_dummies = mlb.fit_transform(df['category_list'])\n",
    "\n",
    "# Create a DataFrame with the one-hot encoded categories\n",
    "category_df = pd.DataFrame(category_dummies, columns=mlb.classes_, index=df.index)\n",
    "\n",
    "# Concatenate the new dummy columns to the original DataFrame\n",
    "df = pd.concat([df, category_df], axis=1)\n",
    "\n",
    "# Drop the temporary 'category_list' column if desired\n",
    "df.drop('category_list', axis=1, inplace=True)\n",
    "df.drop('category', axis=1, inplace=True)\n",
    "\n",
    "num_records, num_attributes = df.shape\n",
    "\n",
    "print(\"There are {} data points, each with {} attributes.\". format(num_records, num_attributes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11789885",
   "metadata": {},
   "source": [
    "### Saving the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10b15971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing file './data/train_preprocessed.csv' has been deleted.\n",
      "DataFrame has been saved to './data/train_preprocessed.csv'.\n"
     ]
    }
   ],
   "source": [
    "file_name = './data/train_preprocessed.csv'\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(file_name):\n",
    "    # Delete the file\n",
    "    os.remove(file_name)\n",
    "    print(f\"Existing file '{file_name}' has been deleted.\")\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "df.to_csv(file_name, index=False)\n",
    "print(f\"DataFrame has been saved to '{file_name}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bc5a63",
   "metadata": {},
   "source": [
    "## Data Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a1b610-13c2-4709-9ad9-cb3482c3d209",
   "metadata": {},
   "source": [
    "### Load preprocessed training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6e80ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 24258 data points in training data, each with 13 attributes.\n"
     ]
    }
   ],
   "source": [
    "# Load file into pandas dataframe, we saved our preprocessed file at path 'output_file'\n",
    "training_file = './data/train_preprocessed.csv'\n",
    "df = pd.read_csv(training_file)\n",
    "\n",
    "columns_to_keep = [\n",
    "    'make',\n",
    "    'model',\n",
    "    'manufactured',\n",
    "    'reg_year',\n",
    "    'type_of_vehicle',\n",
    "    'dereg_value',\n",
    "    'depreciation',\n",
    "    'power',\n",
    "    'coe',\n",
    "    'arf',\n",
    "    'omv',\n",
    "    'price',\n",
    "    'road_tax'\n",
    "]\n",
    "\n",
    "df = df[columns_to_keep]\n",
    "columns_to_keep = [col for col in df.columns if col != 'price']\n",
    "\n",
    "num_records, num_attributes = df.shape\n",
    "print(\"There are {} data points in training data, each with {} attributes.\". format(num_records, num_attributes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81365da5-d652-4c94-9cb9-0bdbc430dc9e",
   "metadata": {},
   "source": [
    "### Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e21a079-8608-48b9-bd9e-389eb321d103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10000 data points, each with 29 attributes.\n",
      "There are 10000 data points in test data, each with 12 attributes.\n"
     ]
    }
   ],
   "source": [
    "test_file = './data/test.csv'\n",
    "df_test = pd.read_csv(test_file)\n",
    "\n",
    "df_test['reg_date'] = pd.to_datetime(df_test['reg_date'], format='%d-%b-%Y')\n",
    "df_test['reg_year'] = df_test['reg_date'].dt.year\n",
    "df_test = df_test.drop(columns=['reg_date'])\n",
    "\n",
    "num_records, num_attributes = df_test.shape\n",
    "print(\"There are {} data points, each with {} attributes.\". format(num_records, num_attributes))\n",
    "\n",
    "categorical_columns = [\n",
    "    'make',\n",
    "    'model',\n",
    "    'type_of_vehicle',\n",
    "    'transmission',\n",
    "]\n",
    "\n",
    "for column in categorical_columns:\n",
    "    df_test[column] = le.fit_transform(df_test[column])\n",
    "\n",
    "df_test = df_test[columns_to_keep]\n",
    "\n",
    "\n",
    "num_records, num_attributes = df_test.shape\n",
    "print(\"There are {} data points in test data, each with {} attributes.\". format(num_records, num_attributes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c681594f-3de4-4511-8154-437e10dee1b6",
   "metadata": {},
   "source": [
    "### Mining code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c22a91f-729c-4a22-a443-0f05af43f099",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.DataMining import split_dataframe, split_dataframe_flex\n",
    "from util.DataMining import (\n",
    "    RandomForestMining,\n",
    "    RandomForestMiningByModel,\n",
    "    GradientBoostingMining,\n",
    "    LinearRegressionMining,\n",
    "    LinearRegressionMiningByModel,\n",
    "    CombinedDataMiningRandomForestAndLinearRegression\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03355739-324a-4a13-a77a-bf301417c9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Random Forest:  20%|█████████████████▌                                                                      | 1/5 [00:26<01:44, 26.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running not in develop mode\n",
      "RMSE on test data: 25451.98517130758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Random Forest:  40%|███████████████████████████████████▏                                                    | 2/5 [00:52<01:18, 26.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running not in develop mode\n",
      "RMSE on test data: 27782.010908891385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Random Forest:  60%|████████████████████████████████████████████████████▊                                   | 3/5 [01:19<00:53, 26.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running not in develop mode\n",
      "RMSE on test data: 19919.584039940735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Random Forest:  80%|██████████████████████████████████████████████████████████████████████▍                 | 4/5 [01:45<00:26, 26.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running not in develop mode\n",
      "RMSE on test data: 18179.631341011085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Random Forest: 100%|████████████████████████████████████████████████████████████████████████████████████████| 5/5 [02:11<00:00, 26.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running not in develop mode\n",
      "RMSE on test data: 17300.451200995492\n",
      "Average RMSE: 21727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_times, rmse_sum = 5, 0\n",
    "for i in tqdm(range(run_times), desc='Running Random Forest'):\n",
    "    target_col = 'price'\n",
    "    x_train, x_test, y_train, y_test = split_dataframe(df, target_col)\n",
    "    rmse_sum += RandomForestMining(x_train, x_test, y_train, y_test)\n",
    "print('Average RMSE:', round(rmse_sum / run_times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30ed9a99-0b30-48f0-8361-060f317f7b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = df.drop(columns=['price']), df['price']\n",
    "x_test = df_test[x_train.columns]\n",
    "\n",
    "res = RandomForestMining(x_train, x_test, y_train, dev=True)\n",
    "res.to_csv('./data/res.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5469d35d-cf27-470e-97ab-9e4f0b67152d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
