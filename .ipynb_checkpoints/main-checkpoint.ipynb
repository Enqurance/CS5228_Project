{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9139239be4d3d26a",
   "metadata": {},
   "source": [
    "<img src=\"images/img.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a5ce1aa953b915",
   "metadata": {},
   "source": [
    "# CS5228 Project, Group 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5287322f-3943-4058-8448-970302c9ff79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f32f076115be79",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "In this part, we are going to perform some data preprocessing steps. This may include:\n",
    "* Data cleaning: handle missing values, duplicates, inconsistant or invalid vallues, outliers\n",
    "\n",
    "* Data reduction: reduce number of attributes, reduce number of attribute values\n",
    "\n",
    "* Data transformation: attribute construction, normalization\n",
    "\n",
    "* Data discretization: encode to numerical attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773fb74f",
   "metadata": {},
   "source": [
    "### Setting up the Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a8c6d3e9adfc751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a084ae15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 25000 data points in training data, each with 30 attributes.\n"
     ]
    }
   ],
   "source": [
    "# Load file into pandas dataframe\n",
    "df = pd.read_csv('./data/train.csv')\n",
    "\n",
    "num_records, num_attributes = df.shape\n",
    "print(\"There are {} data points in training data, each with {} attributes.\". format(num_records, num_attributes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd41045",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389b04ed",
   "metadata": {},
   "source": [
    "Before data cleaning, remove the known attributes that are not meaningful to our prediction model:\n",
    "  * Meaningless idendifier: listing_id \n",
    "  * Attributes in free text: title, description, features, accessories\n",
    "  * Attribute with the same value: eco_category, indicative_price\n",
    "  * Attribute unlikely to affect price: curb_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c02d46d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 25000 data points in training data, each with 20 attributes.\n"
     ]
    }
   ],
   "source": [
    "columns_to_drop = [\n",
    "    'listing_id',          # Meaningless identifier\n",
    "    'title',               # Attributes in free text\n",
    "    'description',\n",
    "    'features',\n",
    "    'accessories',\n",
    "    'eco_category',        # Attribute with the same value\n",
    "    'indicative_price',\n",
    "    'curb_weight',         # Attribute unlikely to affect price\n",
    "    'original_reg_date',\n",
    "    'lifespan',\n",
    "]\n",
    "\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "\n",
    "num_records, num_attributes = df.shape\n",
    "print(\"There are {} data points in training data, each with {} attributes.\". format(num_records, num_attributes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde0aaf8",
   "metadata": {},
   "source": [
    "### Handle Missing Values\n",
    "Firstly, for each of the columns with missing value, check the number of rows with NaN values.\n",
    "There are 3 scenarios:\n",
    "1. NaN value is the major (e.g. fuel_type has 19121 rows with NaN values), we remove the corresponding attritubes.\n",
    "2. NaN value is the minor. We can choose to fill or delete related data points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "834807be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data\n",
      "Column 'make' has 1316 rows with NaN values.\n",
      "Column 'fuel_type' has 19121 rows with NaN values.\n",
      "Column 'manufactured' has 7 rows with NaN values.\n",
      "Column 'power' has 2640 rows with NaN values.\n",
      "Column 'engine_cap' has 596 rows with NaN values.\n",
      "Column 'mileage' has 5304 rows with NaN values.\n",
      "Column 'no_of_owners' has 18 rows with NaN values.\n",
      "Column 'depreciation' has 507 rows with NaN values.\n",
      "Column 'road_tax' has 2632 rows with NaN values.\n",
      "Column 'dereg_value' has 220 rows with NaN values.\n",
      "Column 'omv' has 64 rows with NaN values.\n",
      "Column 'arf' has 174 rows with NaN values.\n",
      "Column 'opc_scheme' has 24838 rows with NaN values.\n"
     ]
    }
   ],
   "source": [
    "columns_to_check = [\n",
    "    'make',\n",
    "    'fuel_type',\n",
    "    'manufactured',\n",
    "    'power',\n",
    "    'engine_cap',\n",
    "    'mileage',\n",
    "    'no_of_owners',\n",
    "    'depreciation',\n",
    "    'road_tax',\n",
    "    'dereg_value',\n",
    "    'omv',\n",
    "    'arf',\n",
    "    'opc_scheme'\n",
    "]\n",
    "\n",
    "# Calculate the number of NaN values in each specified column\n",
    "nan_counts = df[columns_to_check].isna().sum()\n",
    "\n",
    "# Print the number of NaN values for each column\n",
    "print('Training data')\n",
    "for column, count in nan_counts.items():\n",
    "    print(f\"Column '{column}' has {count} rows with NaN values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116b0e50",
   "metadata": {},
   "source": [
    "We delete attributes with TOO many NaN value here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da6b4bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop_nan = [\n",
    "    'fuel_type',\n",
    "    'opc_scheme'\n",
    "]\n",
    "\n",
    "df = df.drop(columns=columns_to_drop_nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea8a0de",
   "metadata": {},
   "source": [
    "Then we try to fill up other missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfc6a526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values after handling:  0\n"
     ]
    }
   ],
   "source": [
    "from util.DataPreprocess import HandlingMissingValues\n",
    "\n",
    "df = HandlingMissingValues(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eafaf03",
   "metadata": {},
   "source": [
    "### Remove Exact Duplicates\n",
    "We remove duplicated data points here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8caa0e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 24258 data points in training data, each with 18 attributes.\n"
     ]
    }
   ],
   "source": [
    "df = df.drop_duplicates()\n",
    "\n",
    "num_records, num_attributes = df.shape\n",
    "print(\"There are {} data points in training data, each with {} attributes.\". format(num_records, num_attributes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84de8476-6545-4f89-ad57-b891986a9045",
   "metadata": {},
   "source": [
    "### Merge rows with fewer data points on specific attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3a7ec8a-8007-4aec-b501-2b4bfb4192ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 2\n",
    "\n",
    "value_counts = df['make'].value_counts()\n",
    "categories_to_replace = value_counts[value_counts < threshold].index\n",
    "df['make'] = df['make'].replace(categories_to_replace, 'others')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28900a18-8bf3-4516-b663-c00533fde54f",
   "metadata": {},
   "source": [
    "### Transform categorical value to numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "829f7d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\n",
    "    'make',\n",
    "    'model',\n",
    "    'type_of_vehicle',\n",
    "    'transmission',\n",
    "]\n",
    "\n",
    "encode_dict = {}\n",
    "le = LabelEncoder()\n",
    "for column in categorical_columns:\n",
    "    df[column] = le.fit_transform(df[column])\n",
    "    encode_dict[column] = {str(label): int(index) for index, label in enumerate(le.classes_)}\n",
    "\n",
    "with open('./data/encode.json', 'w') as file:\n",
    "    json.dump(encode_dict, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a273d568",
   "metadata": {},
   "source": [
    "### Transform date time attributes to numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f580a7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 24258 data points, each with 18 attributes.\n"
     ]
    }
   ],
   "source": [
    "df['reg_date'] = pd.to_datetime(df['reg_date'], format='%d-%b-%Y')\n",
    "df['reg_year'] = df['reg_date'].dt.year\n",
    "df = df.drop(columns=['reg_date'])\n",
    "\n",
    "num_records, num_attributes = df.shape\n",
    "print(\"There are {} data points, each with {} attributes.\". format(num_records, num_attributes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a29cd1d",
   "metadata": {},
   "source": [
    "### Handle category attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a43ae4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique categories: 15\n",
      "Unique categories: {'electric cars', 'opc car', 'rare & exotic', 'consignment car', 'almost new car', 'imported used vehicle', 'parf car', 'premium ad car', 'vintage cars', 'coe car', 'direct owner sale', 'hybrid cars', 'low mileage car', 'sgcarmart warranty cars', 'sta evaluated car'}\n",
      "There are 24258 data points, each with 32 attributes.\n"
     ]
    }
   ],
   "source": [
    "from util.DataPreprocess import HandlingCategoryAttribute\n",
    "\n",
    "df = HandlingCategoryAttribute(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a55c35-f9f7-42ed-a7aa-1e3edfffa7d4",
   "metadata": {},
   "source": [
    "### Remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c95ae45d-0846-4e1b-9f4d-2608349504b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from util.DataPreprocess import OutlierRemoval\n",
    "\n",
    "# df = OutlierRemoval(df, 'model', 'price')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11789885",
   "metadata": {},
   "source": [
    "### Saving the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10b15971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing file './data/train_preprocessed.csv' has been deleted.\n",
      "DataFrame has been saved to './data/train_preprocessed.csv'.\n"
     ]
    }
   ],
   "source": [
    "file_name = './data/train_preprocessed.csv'\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(file_name):\n",
    "    # Delete the file\n",
    "    os.remove(file_name)\n",
    "    print(f\"Existing file '{file_name}' has been deleted.\")\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "df.to_csv(file_name, index=False)\n",
    "print(f\"DataFrame has been saved to '{file_name}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bc5a63",
   "metadata": {},
   "source": [
    "## Data Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a1b610-13c2-4709-9ad9-cb3482c3d209",
   "metadata": {},
   "source": [
    "### Load preprocessed training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e6e80ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 24258 data points in training data, each with 17 attributes.\n"
     ]
    }
   ],
   "source": [
    "# Load file into pandas dataframe, we saved our preprocessed file at path 'output_file'\n",
    "training_file = './data/train_preprocessed.csv'\n",
    "df = pd.read_csv(training_file)\n",
    "\n",
    "columns_to_keep = [\n",
    "    'model',\n",
    "    'mileage',\n",
    "    'low mileage car',\n",
    "    'manufactured',\n",
    "    'reg_year',\n",
    "    'type_of_vehicle',\n",
    "    'dereg_value',\n",
    "    'depreciation',\n",
    "    'power',\n",
    "    'coe',\n",
    "    'arf',\n",
    "    'omv',\n",
    "    'price',\n",
    "    'road_tax',\n",
    "    'almost new car',\n",
    "    'coe car',\n",
    "    'parf car',\n",
    "]\n",
    "\n",
    "df = df[columns_to_keep]\n",
    "columns_to_keep = [col for col in df.columns if col != 'price']\n",
    "\n",
    "num_records, num_attributes = df.shape\n",
    "print(\"There are {} data points in training data, each with {} attributes.\". format(num_records, num_attributes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81365da5-d652-4c94-9cb9-0bdbc430dc9e",
   "metadata": {},
   "source": [
    "### Load test data and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "51b11652-d408-4b2d-b7a0-a39b1dafd5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique categories: 15\n",
      "Unique categories: {'electric cars', 'opc car', 'rare & exotic', 'consignment car', 'almost new car', 'imported used vehicle', 'parf car', 'premium ad car', 'vintage cars', 'coe car', 'direct owner sale', 'hybrid cars', 'low mileage car', 'sgcarmart warranty cars', 'sta evaluated car'}\n",
      "There are 24258 data points, each with 17 attributes.\n"
     ]
    }
   ],
   "source": [
    "test_file = './data/test.csv'\n",
    "df_test = pd.read_csv(test_file)\n",
    "\n",
    "df_test['reg_date'] = pd.to_datetime(df_test['reg_date'], format='%d-%b-%Y')\n",
    "df_test['reg_year'] = df_test['reg_date'].dt.year\n",
    "df_test = df_test.drop(columns=['reg_date'])\n",
    "\n",
    "# Replace '-' with an empty string\n",
    "df_test['category'] = df_test['category'].replace('-', '')\n",
    "\n",
    "# Split the 'category' column into lists\n",
    "df_test['category_list'] = df_test['category'].str.split(', ')\n",
    "\n",
    "# Handle empty strings by replacing them with empty lists\n",
    "df_test['category_list'] = df_test['category_list'].apply(lambda x: [] if x == [''] else x)\n",
    "\n",
    "# Import itertools for flattening lists\n",
    "from itertools import chain\n",
    "\n",
    "# Flatten the list of lists to a single list\n",
    "all_categories = list(chain.from_iterable(df_test['category_list']))\n",
    "\n",
    "# Get the unique categories\n",
    "unique_categories = set(all_categories)\n",
    "\n",
    "# Print the number of unique categories\n",
    "print(f\"Number of unique categories: {len(unique_categories)}\")\n",
    "print(\"Unique categories:\", unique_categories)\n",
    "\n",
    "# Initialize the MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "# Fit and transform the category lists\n",
    "category_dummies = mlb.fit_transform(df_test['category_list'])\n",
    "\n",
    "# Create a DataFrame with the one-hot encoded categories\n",
    "category_df = pd.DataFrame(category_dummies, columns=mlb.classes_, index=df_test.index)\n",
    "\n",
    "# Concatenate the new dummy columns to the original DataFrame\n",
    "df_test = pd.concat([df_test, category_df], axis=1)\n",
    "\n",
    "# Drop the temporary 'category_list' column if desired\n",
    "df_test.drop('category_list', axis=1, inplace=True)\n",
    "df_test.drop('category', axis=1, inplace=True)\n",
    "\n",
    "num_records, num_attributes = df.shape\n",
    "\n",
    "print(\"There are {} data points, each with {} attributes.\". format(num_records, num_attributes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55faa8dc-6231-4b7d-b1de-bb0fc9405e03",
   "metadata": {},
   "source": [
    "### Data Augmentation, copy rows with less than 5 samples by group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1ff32297-3b2c-4a56-94ae-4394247dcf4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 40290 data points after augmentation, each with 17 attributes.\n"
     ]
    }
   ],
   "source": [
    "from util.DataPreprocess import DataAugmentation\n",
    "\n",
    "df_aug = DataAugmentation(df)\n",
    "\n",
    "num_records, num_attributes = df_aug.shape\n",
    "print(\"There are {} data points after augmentation, each with {} attributes.\". format(num_records, num_attributes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e85bd5-970c-425c-a0f0-dce631a8d3a7",
   "metadata": {},
   "source": [
    "### Select attributes on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6e21a079-8608-48b9-bd9e-389eb321d103",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10000 data points, each with 43 attributes.\n",
      "fd7jjma\n",
      "kluger\n",
      "phaeton\n",
      "hkl6540\n",
      "tourer\n",
      "lt434p\n",
      "sh1eema\n",
      "a160\n",
      "sh1eema\n",
      "cx-7\n",
      "s350d\n",
      "colorado\n",
      "slyphy\n",
      "vanguard\n",
      "princess\n",
      "bb\n",
      "ya\n",
      "fd7jpma\n",
      "e350\n",
      "dolphin\n",
      "e-type\n",
      "fvr90\n",
      "ev\n",
      "cwb45a\n",
      "260e\n",
      "clk230\n",
      "230sl\n",
      "eletre\n",
      "clk280\n",
      "genesis\n",
      "e350\n",
      "clk230\n",
      "p5b\n",
      "artura\n",
      "trajet\n",
      "c350\n",
      "sierra\n",
      "tourer\n",
      "patrol\n",
      "350sl\n",
      "88\n",
      "biturbo\n",
      "flh290\n",
      "meriva\n",
      "artura\n",
      "tarraco\n",
      "cyz52r\n",
      "mgb\n",
      "seven\n",
      "fs1elkd\n",
      "fvr90\n",
      "924\n",
      "midget\n",
      "k94ib4x2\n",
      "daewoo\n",
      "good\n",
      "2002\n",
      "clk280\n",
      "sl500\n",
      "xml6772\n",
      "captiva\n",
      "3336k\n",
      "td\n",
      "b7r\n",
      "arnage\n",
      "a170\n",
      "3000gt\n",
      "midget\n",
      "1750\n",
      "tong\n",
      "gh8jrka\n",
      "brooklands\n",
      "9-5\n",
      "alpine\n",
      "924\n",
      "348\n",
      "vanguard\n",
      "88\n",
      "i40\n",
      "2600\n",
      "exige\n",
      "mgb\n",
      "sl280\n",
      "sh1eema\n",
      "356b\n",
      "xml6772\n",
      "safari\n",
      "e280\n",
      "ds\n",
      "e-golf\n",
      "sprinter\n",
      "sl280\n",
      "midget\n",
      "300gd\n",
      "midget\n",
      "dolly\n",
      "62\n",
      "4\n",
      "There are 10000 data points in test data, each with 16 attributes.\n"
     ]
    }
   ],
   "source": [
    "num_records, num_attributes = df_test.shape\n",
    "print(\"There are {} data points, each with {} attributes.\". format(num_records, num_attributes))\n",
    "\n",
    "categorical_columns = [\n",
    "    'make',\n",
    "    'model',\n",
    "    'type_of_vehicle',\n",
    "    'transmission',\n",
    "]\n",
    "\n",
    "with open('./data/encode.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "for col, cate_dict in data.items():\n",
    "    if col in df_test.columns:\n",
    "        for index, row in df_test.iterrows():\n",
    "            if row['model'] not in cate_dict.keys() and col == 'model':\n",
    "                print(row['model'])\n",
    "\n",
    "df_test = df_test[columns_to_keep]\n",
    "\n",
    "num_records, num_attributes = df_test.shape\n",
    "print(\"There are {} data points in test data, each with {} attributes.\". format(num_records, num_attributes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e1608973-e16f-4b27-9360-283e5af48333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data\n",
      "model                98\n",
      "mileage            2166\n",
      "low mileage car       0\n",
      "manufactured          3\n",
      "reg_year              0\n",
      "type_of_vehicle       0\n",
      "dereg_value          83\n",
      "depreciation        201\n",
      "power              1086\n",
      "coe                   0\n",
      "arf                  65\n",
      "omv                  29\n",
      "road_tax           1082\n",
      "almost new car        0\n",
      "coe car               0\n",
      "parf car              0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "nan_counts = df_test[df_test.columns].isna().sum()\n",
    "\n",
    "# Print the number of NaN values for each column\n",
    "print('Test data')\n",
    "print(nan_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd996336-d74b-4e21-b82a-dcd20d42cdcc",
   "metadata": {},
   "source": [
    "### Check if train data has all models in test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d57e5e92-f7c4-454d-8653-d515f7754e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df does not include {np.float64(nan)}\n"
     ]
    }
   ],
   "source": [
    "models_in_df = set(df['model'].unique())\n",
    "models_in_df_test = set(df_test['model'].unique())\n",
    "\n",
    "if models_in_df_test.issubset(models_in_df):\n",
    "    print(\"df includes all models in df_test\")\n",
    "else:\n",
    "    missing_models = models_in_df_test - models_in_df\n",
    "    print(\"df does not include\", missing_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c681594f-3de4-4511-8154-437e10dee1b6",
   "metadata": {},
   "source": [
    "### Mining code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4c22a91f-729c-4a22-a443-0f05af43f099",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.DataMining import split_dataframe, split_dataframe_flex\n",
    "from util.DataMining import (\n",
    "    RandomForestMining,\n",
    "    RandomForestMiningByModel,\n",
    "    GradientBoostingMining,\n",
    "    LinearRegressionMining,\n",
    "    LinearRegressionMiningByModel,\n",
    "    CombinedDataMiningRandomForestAndLinearRegression\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "03355739-324a-4a13-a77a-bf301417c9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_times, rmse_sum = 5, 0\n",
    "# for i in tqdm(range(run_times), desc='Running Random Forest'):\n",
    "#     target_col = 'price'\n",
    "#     x_train, x_test, y_train, y_test = split_dataframe(df, target_col)\n",
    "#     rmse_sum += RandomForestMining(x_train, x_test, y_train, y_test)\n",
    "# print('Average RMSE:', round(rmse_sum / run_times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5469d35d-cf27-470e-97ab-9e4f0b67152d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Random Forest:   0%|                                                                                                | 0/5 [00:00<?, ?it/s]/Users/enqurance/Desktop/NUS/Sem1_AY2024-2025/CS5228/Projects/DataMining/util/DataMining.py:120: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y_pred_df = pd.concat([y_pred_df, temp_df], ignore_index=True)\n",
      "Running Random Forest:  20%|█████████████████▌                                                                      | 1/5 [00:43<02:54, 43.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   model  Prediction\n",
      "0  731.0   161254.76\n",
      "1  564.0   285340.00\n",
      "2  372.0    53411.24\n",
      "3  533.0    20509.92\n",
      "4  483.0    77500.00\n",
      "          price  model\n",
      "19080  155800.0    731\n",
      "17256  285000.0    564\n",
      "2651    58000.0    372\n",
      "13527   14800.0    533\n",
      "20790   77500.0    483\n",
      "Data saved to results.csv\n",
      "Running not in develop mode\n",
      "RMSE on test data: 13094.611381350667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/enqurance/Desktop/NUS/Sem1_AY2024-2025/CS5228/Projects/DataMining/util/DataMining.py:120: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y_pred_df = pd.concat([y_pred_df, temp_df], ignore_index=True)\n",
      "Running Random Forest:  40%|███████████████████████████████████▏                                                    | 2/5 [01:27<02:11, 43.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   model     Prediction\n",
      "0  705.0   37801.015231\n",
      "1  705.0   48061.636598\n",
      "2  705.0   23247.184314\n",
      "3  232.0  118699.120000\n",
      "4  649.0   58888.000000\n",
      "          price  model\n",
      "23487   38000.0    705\n",
      "15680   46000.0    705\n",
      "19218   22800.0    705\n",
      "33767  118888.0    232\n",
      "28798   58888.0    649\n",
      "Data saved to results.csv\n",
      "Running not in develop mode\n",
      "RMSE on test data: 16004.42931582649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/enqurance/Desktop/NUS/Sem1_AY2024-2025/CS5228/Projects/DataMining/util/DataMining.py:120: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y_pred_df = pd.concat([y_pred_df, temp_df], ignore_index=True)\n",
      "Running Random Forest:  60%|████████████████████████████████████████████████████▊                                   | 3/5 [02:11<01:27, 43.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   model  Prediction\n",
      "0  752.0   270785.00\n",
      "1  116.0    68557.37\n",
      "2  454.0   139800.00\n",
      "3   78.0   180000.00\n",
      "4  610.0   202800.00\n",
      "          price  model\n",
      "38411  270800.0    752\n",
      "16130   62800.0    116\n",
      "25632  139800.0    454\n",
      "27001  180000.0     78\n",
      "33290  202800.0    610\n",
      "Data saved to results.csv\n",
      "Running not in develop mode\n",
      "RMSE on test data: 18052.534842244153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/enqurance/Desktop/NUS/Sem1_AY2024-2025/CS5228/Projects/DataMining/util/DataMining.py:120: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y_pred_df = pd.concat([y_pred_df, temp_df], ignore_index=True)\n",
      "Running Random Forest:  80%|██████████████████████████████████████████████████████████████████████▍                 | 4/5 [02:54<00:43, 43.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   model     Prediction\n",
      "0  142.0   30668.543556\n",
      "1  217.0   64000.000000\n",
      "2  416.0   86652.960000\n",
      "3  187.0   10600.000000\n",
      "4  350.0  166270.600000\n",
      "          price  model\n",
      "19167   26800.0    142\n",
      "28924   64000.0    217\n",
      "17581   98800.0    416\n",
      "25850   10600.0    187\n",
      "15541  138000.0    350\n",
      "Data saved to results.csv\n",
      "Running not in develop mode\n",
      "RMSE on test data: 13608.556815960093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/enqurance/Desktop/NUS/Sem1_AY2024-2025/CS5228/Projects/DataMining/util/DataMining.py:120: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y_pred_df = pd.concat([y_pred_df, temp_df], ignore_index=True)\n",
      "Running Random Forest: 100%|████████████████████████████████████████████████████████████████████████████████████████| 5/5 [03:40<00:00, 44.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   model     Prediction\n",
      "0  224.0  149753.060000\n",
      "1  416.0   42011.466714\n",
      "2  752.0  270800.000000\n",
      "3  510.0   18833.706667\n",
      "4  121.0   55149.282167\n",
      "          price  model\n",
      "19801  135800.0    224\n",
      "16895   42500.0    416\n",
      "38397  270800.0    752\n",
      "15312   18800.0    510\n",
      "2914    57800.0    121\n",
      "Data saved to results.csv\n",
      "Running not in develop mode\n",
      "RMSE on test data: 13355.431496767686\n",
      "Average RMSE: 14823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_times, rmse_sum = 1, 0\n",
    "for i in tqdm(range(run_times), desc='Running Random Forest'):\n",
    "    train_drop_cols = ['price']\n",
    "    test_cols = ['price', 'model']\n",
    "    x_train, x_test, y_train, y_test = split_dataframe_flex(df_aug, train_drop_cols, test_cols)\n",
    "    rmse_sum += RandomForestMiningByModel(x_train, x_test, y_train, y_test)\n",
    "print('Average RMSE:', round(rmse_sum / run_times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f5716ffc-b01c-4851-b397-e498964f3afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train, y_train = df.drop(columns=['price']), df['price']\n",
    "# x_test = df_test[x_train.columns]\n",
    "\n",
    "# res = RandomForestMining(x_train, x_test, y_train, dev=True)\n",
    "# res.to_csv('./data/res.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "900b06eb-489d-40c2-8722-61bcd5fd87a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   model   mileage  low mileage car  manufactured  reg_year  type_of_vehicle  \\\n",
      "0  705.0  112000.0                0        2015.0      2015                8   \n",
      "1   36.0  120000.0                1        2007.0      2007                3   \n",
      "2  221.0   43000.0                0        2019.0      2020                6   \n",
      "3  707.0   53300.0                0        2019.0      2019                3   \n",
      "4   36.0  149000.0                0        2015.0      2015                1   \n",
      "\n",
      "   dereg_value  depreciation  power    coe      arf      omv  road_tax  \\\n",
      "0       9582.0       17660.0   96.0  57199   9229.0  19229.0     682.0   \n",
      "1      13644.0       10920.0   79.0  42564  15782.0  14347.0    1113.0   \n",
      "2      54818.0       22120.0  141.0  32801  47809.0  39863.0    1210.0   \n",
      "3      26363.0       13700.0   79.0  29159  15573.0  15573.0     682.0   \n",
      "4      15197.0       14190.0   88.0  56001  13097.0  18097.0     682.0   \n",
      "\n",
      "   almost new car  coe car  parf car  \n",
      "0               0        0         1  \n",
      "1               0        1         0  \n",
      "2               0        0         1  \n",
      "3               0        0         1  \n",
      "4               0        0         1  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/enqurance/Desktop/NUS/Sem1_AY2024-2025/CS5228/Projects/DataMining/util/DataMining.py:120: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  y_pred_df = pd.concat([y_pred_df, temp_df], ignore_index=True)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "np.float64(nan)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m x_test \u001b[38;5;241m=\u001b[39m df_test[x_train\u001b[38;5;241m.\u001b[39mcolumns]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_test\u001b[38;5;241m.\u001b[39mhead())\n\u001b[0;32m----> 5\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mRandomForestMiningByModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m res\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/res_by_model.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/NUS/Sem1_AY2024-2025/CS5228/Projects/DataMining/util/DataMining.py:112\u001b[0m, in \u001b[0;36mRandomForestMiningByModel\u001b[0;34m(x_train, x_test, y_train, y_test, dev)\u001b[0m\n\u001b[1;32m    110\u001b[0m y_pred_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrediction\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, test \u001b[38;5;129;01min\u001b[39;00m x_test\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m--> 112\u001b[0m \tmodel \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    113\u001b[0m \tX \u001b[38;5;241m=\u001b[39m test\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mto_frame()\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    114\u001b[0m \ty_pred_new \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X)\n",
      "\u001b[0;31mKeyError\u001b[0m: np.float64(nan)"
     ]
    }
   ],
   "source": [
    "x_train, y_train = df.drop(columns=['price']), df[['price', 'model']]\n",
    "x_test = df_test[x_train.columns]\n",
    "print(df_test.head())\n",
    "\n",
    "res = RandomForestMiningByModel(x_train, x_test, y_train, dev=True)\n",
    "res.to_csv('./data/res_by_model.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a93b02-347e-45fb-82bd-b1627555e72f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
