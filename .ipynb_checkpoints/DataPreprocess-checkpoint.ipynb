{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9139239be4d3d26a",
   "metadata": {},
   "source": [
    "<img src=\"images/img.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a5ce1aa953b915",
   "metadata": {},
   "source": [
    "# CS5228 Project, Group 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5287322f-3943-4058-8448-970302c9ff79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a8c6d3e9adfc751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f32f076115be79",
   "metadata": {},
   "source": [
    "## Train data preprocess\n",
    "In this part, we are going to perform some data preprocessing steps. This may include:\n",
    "* Data cleaning: handle missing values, duplicates, inconsistant or invalid vallues, outliers\n",
    "\n",
    "* Data reduction: reduce number of attributes, reduce number of attribute values\n",
    "\n",
    "* Data transformation: attribute construction, normalization\n",
    "\n",
    "* Data discretization: encode to numerical attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773fb74f",
   "metadata": {},
   "source": [
    "### Load the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a084ae15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 25000 data points in training data, each with 30 attributes.\n"
     ]
    }
   ],
   "source": [
    "# Load file into pandas dataframe\n",
    "df = pd.read_csv('./data/train.csv')\n",
    "\n",
    "num_records, num_attributes = df.shape\n",
    "print(\"There are {} data points in training data, each with {} attributes.\". format(num_records, num_attributes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd41045",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389b04ed",
   "metadata": {},
   "source": [
    "Before data cleaning, remove the known attributes that are not meaningful to our prediction model:\n",
    "  * Meaningless idendifier: listing_id \n",
    "  * Attributes in free text: title, description, features, accessories\n",
    "  * Attribute with the same value: eco_category, indicative_price\n",
    "  * Attribute unlikely to affect price: curb_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777dbed7",
   "metadata": {},
   "source": [
    "We first drop columns with free text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c02d46d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 25000 data points in training data, each with 20 attributes.\n"
     ]
    }
   ],
   "source": [
    "columns_to_drop = [\n",
    "    'listing_id',          # Meaningless identifier\n",
    "    'description',\n",
    "    'features',\n",
    "    'accessories',\n",
    "    'eco_category',        # Attribute with the same value\n",
    "    'indicative_price',\n",
    "    'curb_weight',         # Attribute unlikely to affect price\n",
    "    'transmission',\n",
    "    'original_reg_date',\n",
    "    'lifespan',\n",
    "]\n",
    "\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "\n",
    "num_records, num_attributes = df.shape\n",
    "print(\"There are {} data points in training data, each with {} attributes.\". format(num_records, num_attributes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde0aaf8",
   "metadata": {},
   "source": [
    "### Handle Missing Values\n",
    "Firstly, for each of the columns with missing value, check the number of rows with NaN values.\n",
    "There are 3 scenarios:\n",
    "1. NaN value is the major (e.g. fuel_type has 19121 rows with NaN values), we remove the corresponding attritubes.\n",
    "2. NaN value is the minor. We can choose to fill or delete related data points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "834807be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data\n",
      "Column 'make' has 1316 rows with NaN values.\n",
      "Column 'fuel_type' has 19121 rows with NaN values.\n",
      "Column 'manufactured' has 7 rows with NaN values.\n",
      "Column 'power' has 2640 rows with NaN values.\n",
      "Column 'engine_cap' has 596 rows with NaN values.\n",
      "Column 'mileage' has 5304 rows with NaN values.\n",
      "Column 'no_of_owners' has 18 rows with NaN values.\n",
      "Column 'depreciation' has 507 rows with NaN values.\n",
      "Column 'road_tax' has 2632 rows with NaN values.\n",
      "Column 'dereg_value' has 220 rows with NaN values.\n",
      "Column 'omv' has 64 rows with NaN values.\n",
      "Column 'arf' has 174 rows with NaN values.\n",
      "Column 'opc_scheme' has 24838 rows with NaN values.\n"
     ]
    }
   ],
   "source": [
    "columns_to_check = [\n",
    "    'make',\n",
    "    'fuel_type',\n",
    "    'manufactured',\n",
    "    'power',\n",
    "    'engine_cap',\n",
    "    'mileage',\n",
    "    'no_of_owners',\n",
    "    'depreciation',\n",
    "    'road_tax',\n",
    "    'dereg_value',\n",
    "    'omv',\n",
    "    'arf',\n",
    "    'opc_scheme'\n",
    "]\n",
    "\n",
    "# Calculate the number of NaN values in each specified column\n",
    "nan_counts = df[columns_to_check].isna().sum()\n",
    "\n",
    "# Print the number of NaN values for each column\n",
    "print('Training data')\n",
    "for column, count in nan_counts.items():\n",
    "    print(f\"Column '{column}' has {count} rows with NaN values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2655f7",
   "metadata": {},
   "source": [
    "We first drop columns with TOO MANY NaN values and unlikely to help prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da6b4bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop_nan = [\n",
    "    'fuel_type',\n",
    "    'opc_scheme',\n",
    "]\n",
    "\n",
    "for col in columns_to_drop_nan:\n",
    "    if col in df.columns:\n",
    "        df = df.drop(columns=[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28900a18-8bf3-4516-b663-c00533fde54f",
   "metadata": {},
   "source": [
    "### Data transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09be75a",
   "metadata": {},
   "source": [
    "Transform date time attributes to numerical values\n",
    "This step is required to fill up the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce15c388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 25000 data points, each with 19 attributes.\n"
     ]
    }
   ],
   "source": [
    "from util.DataPreprocess import CalculateCarAge\n",
    "    \n",
    "\n",
    "df = CalculateCarAge(df)\n",
    "num_records, num_attributes = df.shape\n",
    "print(\"There are {} data points, each with {} attributes.\". format(num_records, num_attributes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05eef72f",
   "metadata": {},
   "source": [
    "Then we fill column 'make' with title, since we notice that make is always covered by title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a1d7a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_set = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if pd.notna(row['make']) and row['make'] in row['title'].lower():\n",
    "        make_set.append(row['make'])\n",
    "        \n",
    "for index, row in df.iterrows():\n",
    "    if pd.isna(row['make']):\n",
    "        for make in make_set:\n",
    "            if make in row['title'].lower():\n",
    "                df.at[index, 'make'] = make.lower()\n",
    "                break\n",
    "                \n",
    "df = df.drop(columns=['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f653cf1f",
   "metadata": {},
   "source": [
    "Then we transform categorical data to numerical data. We first use LabelEncoder to do encoding and save the label mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "829f7d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = './data/test.csv'\n",
    "df_test = pd.read_csv(test_file)\n",
    "\n",
    "categorical_columns = [\n",
    "    'model',\n",
    "    'make',\n",
    "    'type_of_vehicle',\n",
    "]\n",
    "\n",
    "encode_dict = {}\n",
    "le = LabelEncoder()\n",
    "for column in categorical_columns:\n",
    "    df[column] = le.fit_transform(df[column])\n",
    "    encode_dict[column] = {str(label): int(index) for index, label in enumerate(le.classes_)}\n",
    "\n",
    "with open('./data/encode.json', 'w') as file:\n",
    "    json.dump(encode_dict, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26c3855",
   "metadata": {},
   "source": [
    "We then handle category column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9823aef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique categories: 15\n",
      "Unique categories: {'sgcarmart warranty cars', 'electric cars', 'rare & exotic', 'hybrid cars', 'vintage cars', 'sta evaluated car', 'low mileage car', 'direct owner sale', 'imported used vehicle', 'consignment car', 'coe car', 'almost new car', 'premium ad car', 'parf car', 'opc car'}\n",
      "There are 25000 data points, each with 32 attributes.\n",
      "Index(['make', 'model', 'manufactured', 'type_of_vehicle', 'power',\n",
      "       'engine_cap', 'no_of_owners', 'depreciation', 'coe', 'road_tax',\n",
      "       'dereg_value', 'mileage', 'omv', 'arf', 'price', 'reg_year', 'car_age',\n",
      "       'almost new car', 'coe car', 'consignment car', 'direct owner sale',\n",
      "       'electric cars', 'hybrid cars', 'imported used vehicle',\n",
      "       'low mileage car', 'opc car', 'parf car', 'premium ad car',\n",
      "       'rare & exotic', 'sgcarmart warranty cars', 'sta evaluated car',\n",
      "       'vintage cars'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from util.DataPreprocess import HandlingCategoryAttribute\n",
    "\n",
    "if 'category' in df.columns:\n",
    "    df = HandlingCategoryAttribute(df)\n",
    "    \n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22ec92b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data\n",
      "Column 'make' has 0 rows with NaN values.\n",
      "Column 'manufactured' has 7 rows with NaN values.\n",
      "Column 'engine_cap' has 596 rows with NaN values.\n",
      "Column 'mileage' has 5304 rows with NaN values.\n",
      "Column 'no_of_owners' has 18 rows with NaN values.\n",
      "Column 'depreciation' has 507 rows with NaN values.\n",
      "Column 'road_tax' has 2632 rows with NaN values.\n",
      "Column 'dereg_value' has 220 rows with NaN values.\n",
      "Column 'omv' has 64 rows with NaN values.\n",
      "Column 'arf' has 174 rows with NaN values.\n",
      "There are 24993 data points, each with 32 attributes.\n"
     ]
    }
   ],
   "source": [
    "columns_to_check = [\n",
    "    'make',\n",
    "    'manufactured',\n",
    "    'engine_cap',\n",
    "    'mileage',\n",
    "    'no_of_owners',\n",
    "    'depreciation',\n",
    "    'road_tax',\n",
    "    'dereg_value',\n",
    "    'omv',\n",
    "    'arf',\n",
    "]\n",
    "\n",
    "# Calculate the number of NaN values in each specified column\n",
    "nan_counts = df[columns_to_check].isna().sum()\n",
    "\n",
    "# Print the number of NaN values for each column\n",
    "print('Training data')\n",
    "for column, count in nan_counts.items():\n",
    "    print(f\"Column '{column}' has {count} rows with NaN values.\")\n",
    "\n",
    "df = df.dropna(subset=[\n",
    "    'manufactured',\n",
    "])\n",
    "\n",
    "num_records, num_attributes = df.shape\n",
    "print(\"There are {} data points, each with {} attributes.\". format(num_records, num_attributes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea8a0de",
   "metadata": {},
   "source": [
    "### Then we try to fill up other missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfc6a526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['make', 'model', 'manufactured', 'type_of_vehicle', 'power',\n",
      "       'engine_cap', 'no_of_owners', 'depreciation', 'coe', 'road_tax',\n",
      "       'dereg_value', 'mileage', 'omv', 'arf', 'price', 'reg_year', 'car_age',\n",
      "       'almost new car', 'coe car', 'consignment car', 'direct owner sale',\n",
      "       'electric cars', 'hybrid cars', 'imported used vehicle',\n",
      "       'low mileage car', 'opc car', 'parf car', 'premium ad car',\n",
      "       'rare & exotic', 'sgcarmart warranty cars', 'sta evaluated car',\n",
      "       'vintage cars'],\n",
      "      dtype='object')\n",
      "NaN values after handling:  0\n",
      "make                       0\n",
      "model                      0\n",
      "manufactured               0\n",
      "type_of_vehicle            0\n",
      "power                      0\n",
      "engine_cap                 0\n",
      "no_of_owners               0\n",
      "depreciation               0\n",
      "coe                        0\n",
      "road_tax                   0\n",
      "dereg_value                0\n",
      "mileage                    0\n",
      "omv                        0\n",
      "arf                        0\n",
      "price                      0\n",
      "reg_year                   0\n",
      "car_age                    0\n",
      "almost new car             0\n",
      "coe car                    0\n",
      "consignment car            0\n",
      "direct owner sale          0\n",
      "electric cars              0\n",
      "hybrid cars                0\n",
      "imported used vehicle      0\n",
      "low mileage car            0\n",
      "opc car                    0\n",
      "parf car                   0\n",
      "premium ad car             0\n",
      "rare & exotic              0\n",
      "sgcarmart warranty cars    0\n",
      "sta evaluated car          0\n",
      "vintage cars               0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from util.DataPreprocess import HandlingMissingValue\n",
    "from util.DataPreprocess import HandlingMissingValueWithImpute\n",
    "\n",
    "columns = [\n",
    "    'model',\n",
    "    'mileage',\n",
    "    'engine_cap',\n",
    "    'no_of_owners',\n",
    "    'coe', \n",
    "    'road_tax',\n",
    "    'omv',\n",
    "    'arf',\n",
    "    'price',\n",
    "]\n",
    "\n",
    "print(df.columns)\n",
    "\n",
    "df = HandlingMissingValue(df)\n",
    "df = HandlingMissingValueWithImpute(df, columns)\n",
    "total_nulls = df.isnull().sum()\n",
    "print(total_nulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8c1cf5f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>make</th>\n",
       "      <th>model</th>\n",
       "      <th>manufactured</th>\n",
       "      <th>type_of_vehicle</th>\n",
       "      <th>power</th>\n",
       "      <th>engine_cap</th>\n",
       "      <th>no_of_owners</th>\n",
       "      <th>depreciation</th>\n",
       "      <th>coe</th>\n",
       "      <th>road_tax</th>\n",
       "      <th>...</th>\n",
       "      <th>hybrid cars</th>\n",
       "      <th>imported used vehicle</th>\n",
       "      <th>low mileage car</th>\n",
       "      <th>opc car</th>\n",
       "      <th>parf car</th>\n",
       "      <th>premium ad car</th>\n",
       "      <th>rare &amp; exotic</th>\n",
       "      <th>sgcarmart warranty cars</th>\n",
       "      <th>sta evaluated car</th>\n",
       "      <th>vintage cars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43</td>\n",
       "      <td>595.0</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>8</td>\n",
       "      <td>280.0</td>\n",
       "      <td>2995.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>34270.0</td>\n",
       "      <td>48011.0</td>\n",
       "      <td>2380.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51</td>\n",
       "      <td>192.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>2</td>\n",
       "      <td>135.0</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21170.0</td>\n",
       "      <td>47002.0</td>\n",
       "      <td>1202.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29</td>\n",
       "      <td>546.0</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>4</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2354.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12520.0</td>\n",
       "      <td>50355.0</td>\n",
       "      <td>2442.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88</td>\n",
       "      <td>156.0</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>3</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1598.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10140.0</td>\n",
       "      <td>27571.0</td>\n",
       "      <td>1113.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44</td>\n",
       "      <td>398.0</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>2</td>\n",
       "      <td>183.0</td>\n",
       "      <td>2995.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13690.0</td>\n",
       "      <td>48479.0</td>\n",
       "      <td>3570.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   make  model  manufactured  type_of_vehicle  power  engine_cap  \\\n",
       "0    43  595.0        2018.0                8  280.0      2995.0   \n",
       "1    51  192.0        2017.0                2  135.0      1991.0   \n",
       "2    29  546.0        2007.0                4  118.0      2354.0   \n",
       "3    88  156.0        2008.0                3   80.0      1598.0   \n",
       "4    44  398.0        2006.0                2  183.0      2995.0   \n",
       "\n",
       "   no_of_owners  depreciation      coe  road_tax  ...  hybrid cars  \\\n",
       "0           2.0       34270.0  48011.0    2380.0  ...            0   \n",
       "1           2.0       21170.0  47002.0    1202.0  ...            0   \n",
       "2           3.0       12520.0  50355.0    2442.0  ...            0   \n",
       "3           3.0       10140.0  27571.0    1113.0  ...            0   \n",
       "4           6.0       13690.0  48479.0    3570.0  ...            0   \n",
       "\n",
       "   imported used vehicle  low mileage car  opc car  parf car  premium ad car  \\\n",
       "0                      0                0        0         1               0   \n",
       "1                      0                0        0         1               1   \n",
       "2                      0                1        0         0               1   \n",
       "3                      0                0        0         0               1   \n",
       "4                      0                0        0         0               1   \n",
       "\n",
       "   rare & exotic  sgcarmart warranty cars  sta evaluated car  vintage cars  \n",
       "0              0                        0                  0             0  \n",
       "1              0                        0                  0             0  \n",
       "2              0                        0                  0             0  \n",
       "3              0                        0                  0             0  \n",
       "4              0                        0                  0             0  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eafaf03",
   "metadata": {},
   "source": [
    "### Remove Exact Duplicates\n",
    "We remove duplicated data points here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8caa0e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 24415 data points in training data, each with 32 attributes.\n"
     ]
    }
   ],
   "source": [
    "df = df.drop_duplicates()\n",
    "\n",
    "num_records, num_attributes = df.shape\n",
    "print(\"There are {} data points in training data, each with {} attributes.\". format(num_records, num_attributes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a55c35-f9f7-42ed-a7aa-1e3edfffa7d4",
   "metadata": {},
   "source": [
    "### Data Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c95ae45d-0846-4e1b-9f4d-2608349504b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.DataPreprocess import DataCalculation\n",
    "\n",
    "df = DataCalculation(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11789885",
   "metadata": {},
   "source": [
    "### Save the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10b15971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing file './data/train_preprocessed_impute.csv' has been deleted.\n",
      "DataFrame has been saved to './data/train_preprocessed_impute.csv'.\n"
     ]
    }
   ],
   "source": [
    "file_name = './data/train_preprocessed_impute.csv'\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(file_name):\n",
    "    # Delete the file\n",
    "    os.remove(file_name)\n",
    "    print(f\"Existing file '{file_name}' has been deleted.\")\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "df.to_csv(file_name, index=False)\n",
    "print(f\"DataFrame has been saved to '{file_name}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a1b610-13c2-4709-9ad9-cb3482c3d209",
   "metadata": {},
   "source": [
    "### Load preprocessed training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6e80ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['make', 'model', 'manufactured', 'type_of_vehicle', 'power',\n",
      "       'engine_cap', 'no_of_owners', 'depreciation', 'coe', 'road_tax',\n",
      "       'dereg_value', 'mileage', 'omv', 'arf', 'price', 'reg_year', 'car_age',\n",
      "       'almost new car', 'coe car', 'consignment car', 'direct owner sale',\n",
      "       'electric cars', 'hybrid cars', 'imported used vehicle',\n",
      "       'low mileage car', 'opc car', 'parf car', 'premium ad car',\n",
      "       'rare & exotic', 'sgcarmart warranty cars', 'sta evaluated car',\n",
      "       'vintage cars', 'omv_arf_ratio', 'dereg_coe_ratio'],\n",
      "      dtype='object')\n",
      "There are 24415 data points in training data, each with 34 attributes.\n"
     ]
    }
   ],
   "source": [
    "# Load file into pandas dataframe, we saved our preprocessed file at path 'output_file'\n",
    "training_file = './data/train_preprocessed_impute.csv'\n",
    "df = pd.read_csv(training_file)\n",
    "\n",
    "columns_to_keep = [col for col in df.columns if col != 'price']\n",
    "\n",
    "print(df.columns)\n",
    "num_records, num_attributes = df.shape\n",
    "print(\"There are {} data points in training data, each with {} attributes.\". format(num_records, num_attributes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77da5708",
   "metadata": {},
   "source": [
    "### Data Augmentation, copy rows with less than 5 samples by group. The augmentated data is used only for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1aa49ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 40693 data points after augmentation, each with 34 attributes.\n"
     ]
    }
   ],
   "source": [
    "from util.DataPreprocess import DataAugmentation\n",
    "\n",
    "df_aug = DataAugmentation(df)\n",
    "\n",
    "num_records, num_attributes = df_aug.shape\n",
    "print(\"There are {} data points after augmentation, each with {} attributes.\". format(num_records, num_attributes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca582db1",
   "metadata": {},
   "source": [
    "### Save the augmentation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4d43381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing file './data/train_preprocessed_augmentation.csv' has been deleted.\n",
      "DataFrame has been saved to './data/train_preprocessed_augmentation.csv'.\n"
     ]
    }
   ],
   "source": [
    "file_name = './data/train_preprocessed_augmentation.csv'\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(file_name):\n",
    "    # Delete the file\n",
    "    os.remove(file_name)\n",
    "    print(f\"Existing file '{file_name}' has been deleted.\")\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "df_aug.to_csv(file_name, index=False)\n",
    "print(f\"DataFrame has been saved to '{file_name}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b5da8d",
   "metadata": {},
   "source": [
    "## Preprocess test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81365da5-d652-4c94-9cb9-0bdbc430dc9e",
   "metadata": {},
   "source": [
    "Load test data and preprocess. We also need training data to help here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "51b11652-d408-4b2d-b7a0-a39b1dafd5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = './data/train_preprocessed_impute.csv'\n",
    "df = pd.read_csv(train_file)\n",
    "\n",
    "test_file = './data/test.csv'\n",
    "df_test = pd.read_csv(test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71806687",
   "metadata": {},
   "source": [
    "We first drop columns with free text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5c1519ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10000 data points in training data, each with 19 attributes.\n"
     ]
    }
   ],
   "source": [
    "columns_to_drop = [\n",
    "    'listing_id',          # Meaningless identifier\n",
    "    'description',\n",
    "    'features',\n",
    "    'accessories',\n",
    "    'eco_category',        # Attribute with the same value\n",
    "    'indicative_price',\n",
    "    'curb_weight',         # Attribute unlikely to affect price\n",
    "    'transmission',\n",
    "    'original_reg_date',\n",
    "    'lifespan',\n",
    "]\n",
    "\n",
    "df_test = df_test.drop(columns=columns_to_drop)\n",
    "\n",
    "num_records, num_attributes = df_test.shape\n",
    "print(\"There are {} data points in training data, each with {} attributes.\". format(num_records, num_attributes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f062d2a",
   "metadata": {},
   "source": [
    "### Calculate car age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8e6ecdb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10000 data points, each with 20 attributes.\n"
     ]
    }
   ],
   "source": [
    "from util.DataPreprocess import CalculateCarAge\n",
    "df_test = CalculateCarAge(df_test)\n",
    "\n",
    "num_records, num_attributes = df_test.shape\n",
    "print(\"There are {} data points, each with {} attributes.\". format(num_records, num_attributes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35084373",
   "metadata": {},
   "source": [
    "### Convert category data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "434bdf6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique categories: 15\n",
      "Unique categories: {'sgcarmart warranty cars', 'opc car', 'electric cars', 'hybrid cars', 'rare & exotic', 'vintage cars', 'sta evaluated car', 'low mileage car', 'direct owner sale', 'imported used vehicle', 'consignment car', 'coe car', 'premium ad car', 'parf car', 'almost new car'}\n",
      "There are 10000 data points, each with 34 attributes.\n"
     ]
    }
   ],
   "source": [
    "from util.DataPreprocess import HandlingCategoryAttribute\n",
    "\n",
    "if 'category' in df_test.columns:\n",
    "    df_test = HandlingCategoryAttribute(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bebacb",
   "metadata": {},
   "source": [
    "### Handle missing values on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcae343",
   "metadata": {},
   "source": [
    "We first drop columns with TOO MANY NaN values and unlikely to help prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6dad8f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop_nan = [\n",
    "    'fuel_type',\n",
    "    'opc_scheme',\n",
    "]\n",
    "\n",
    "for col in columns_to_drop_nan:\n",
    "    if col in df_test.columns:\n",
    "        df_test = df_test.drop(columns=[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54524029",
   "metadata": {},
   "source": [
    "We also fill column 'make' with 'title' here as we did before on train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6005626c",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_set = []\n",
    "\n",
    "for index, row in df_test.iterrows():\n",
    "    if pd.notna(row['make']) and row['make'] in row['title'].lower():\n",
    "        make_set.append(row['make'])\n",
    "        \n",
    "for index, row in df_test.iterrows():\n",
    "    if pd.isna(row['make']):\n",
    "        for make in make_set:\n",
    "            if make in row['title'].lower():\n",
    "                df_test.at[index, 'make'] = make.lower()\n",
    "                break\n",
    "                \n",
    "df_test = df_test.drop(columns=['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d2786c79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data\n",
      "make                          0\n",
      "model                         0\n",
      "manufactured                  3\n",
      "type_of_vehicle               0\n",
      "power                      1086\n",
      "engine_cap                  235\n",
      "no_of_owners                  8\n",
      "depreciation                201\n",
      "coe                           0\n",
      "road_tax                   1082\n",
      "dereg_value                  83\n",
      "mileage                    2166\n",
      "omv                          29\n",
      "arf                          65\n",
      "reg_year                      0\n",
      "car_age                       0\n",
      "almost new car                0\n",
      "coe car                       0\n",
      "consignment car               0\n",
      "direct owner sale             0\n",
      "electric cars                 0\n",
      "hybrid cars                   0\n",
      "imported used vehicle         0\n",
      "low mileage car               0\n",
      "opc car                       0\n",
      "parf car                      0\n",
      "premium ad car                0\n",
      "rare & exotic                 0\n",
      "sgcarmart warranty cars       0\n",
      "sta evaluated car             0\n",
      "vintage cars                  0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "columns_to_check = [\n",
    "    'make',\n",
    "    'model',\n",
    "    'manufactured',\n",
    "    'engine_cap',\n",
    "    'no_of_owners',\n",
    "    'depreciation',\n",
    "    'road_tax',\n",
    "    'mileage',\n",
    "    'dereg_value'\n",
    "]\n",
    "\n",
    "# Calculate the number of NaN values in each specified column\n",
    "nan_counts = df_test[columns_to_check].isna().sum()\n",
    "\n",
    "# Print the number of NaN values for each column\n",
    "print('Test data')\n",
    "total_nulls = df_test.isnull().sum()\n",
    "print(total_nulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d2768fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.DataPreprocess import HandlingCategoryAttribute\n",
    "\n",
    "if 'category' in df_test.columns:\n",
    "    df_test = HandlingCategoryAttribute(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6508fc75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['make', 'model', 'manufactured', 'type_of_vehicle', 'power',\n",
      "       'engine_cap', 'no_of_owners', 'depreciation', 'coe', 'road_tax',\n",
      "       'dereg_value', 'mileage', 'omv', 'arf', 'price', 'reg_year', 'car_age',\n",
      "       'almost new car', 'coe car', 'consignment car', 'direct owner sale',\n",
      "       'electric cars', 'hybrid cars', 'imported used vehicle',\n",
      "       'low mileage car', 'opc car', 'parf car', 'premium ad car',\n",
      "       'rare & exotic', 'sgcarmart warranty cars', 'sta evaluated car',\n",
      "       'vintage cars', 'omv_arf_ratio', 'dereg_coe_ratio'],\n",
      "      dtype='object')\n",
      "NaN values after handling:  3964\n"
     ]
    }
   ],
   "source": [
    "from util.DataPreprocess import HandlingMissingValueWithImputeReference\n",
    "from util.DataPreprocess import HandlingMissingValueTest\n",
    "\n",
    "columns = [\n",
    "    'omv',\n",
    "    'arf',\n",
    "    'manufactured',\n",
    "    'engine_cap',\n",
    "    'no_of_owners',\n",
    "    'depreciation',\n",
    "    'road_tax',\n",
    "    'mileage',\n",
    "    'dereg_value'\n",
    "]\n",
    "print(df.columns)\n",
    "\n",
    "df_test = HandlingMissingValueTest(df_test)\n",
    "df_test = HandlingMissingValueWithImputeReference(df_test, df, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2c6f68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_test = HandlingMissingValueWithReference(df, df_test)\n",
    "total_nulls = df_test.isnull().sum()\n",
    "print(total_nulls)\n",
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c331190",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from util.DataPreprocess import DataCalculation\n",
    "\n",
    "df_test = DataCalculation(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e85bd5-970c-425c-a0f0-dce631a8d3a7",
   "metadata": {},
   "source": [
    "### Encode attributes on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e21a079-8608-48b9-bd9e-389eb321d103",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_records, num_attributes = df_test.shape\n",
    "print(\"There are {} data points, each with {} attributes.\". format(num_records, num_attributes))\n",
    "\n",
    "categorical_columns = [\n",
    "    'make',\n",
    "    'model',\n",
    "    'type_of_vehicle'\n",
    "]\n",
    "\n",
    "with open('./data/encode.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "new_encodings = {}\n",
    "\n",
    "for col, cate_dict in data.items():\n",
    "    if col in df_test.columns and col in categorical_columns:\n",
    "        original_values = df_test[col].copy()\n",
    "        \n",
    "        df_test[col] = df_test[col].map(cate_dict)\n",
    "        \n",
    "        missing = df_test[col][df_test[col].isna()]\n",
    "        if not missing.empty:\n",
    "            original_missing_values = original_values[missing.index]\n",
    "            unique_missing_values = original_missing_values.unique()\n",
    "            new_encoding_dict = {val: idx for idx, val in enumerate(unique_missing_values, start=max(cate_dict.values()) + 1)}\n",
    "            new_encodings[col] = new_encoding_dict\n",
    "            \n",
    "            df_test[col].fillna(original_missing_values.map(new_encoding_dict), inplace=True)\n",
    "\n",
    "if new_encodings:\n",
    "    print(\"New encodings created for missing values:\")\n",
    "    for col, encodings in new_encodings.items():\n",
    "        print(f\"Column: {col}, New encodings: {encodings}\")\n",
    "else:\n",
    "    print(\"No new encodings were necessary.\")\n",
    "\n",
    "print(df_test.head())\n",
    "\n",
    "num_records, num_attributes = df_test.shape\n",
    "print(f\"There are {num_records} data points in test data, each with {num_attributes} attributes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100bcd12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_test = HandlingMissingValueWithReference(df, df_test)\n",
    "total_nulls = df_test.isnull().sum()\n",
    "print(total_nulls)\n",
    "\n",
    "df_test.to_csv('./data/test_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80083ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28778eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f7dcc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
