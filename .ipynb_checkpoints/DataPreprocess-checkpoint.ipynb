{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9139239be4d3d26a",
   "metadata": {},
   "source": [
    "<img src=\"images/img.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a5ce1aa953b915",
   "metadata": {},
   "source": [
    "# CS5228 Project, Group 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5287322f-3943-4058-8448-970302c9ff79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a8c6d3e9adfc751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f32f076115be79",
   "metadata": {},
   "source": [
    "## Train data preprocess\n",
    "In this part, we are going to perform some data preprocessing steps. This may include:\n",
    "* Data cleaning: handle missing values, duplicates, inconsistant or invalid vallues, outliers\n",
    "\n",
    "* Data reduction: reduce number of attributes, reduce number of attribute values\n",
    "\n",
    "* Data transformation: attribute construction, normalization\n",
    "\n",
    "* Data discretization: encode to numerical attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773fb74f",
   "metadata": {},
   "source": [
    "### Load the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a084ae15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 25000 data points in training data, each with 30 attributes.\n"
     ]
    }
   ],
   "source": [
    "# Load file into pandas dataframe\n",
    "df = pd.read_csv('./data/train.csv')\n",
    "\n",
    "num_records, num_attributes = df.shape\n",
    "print(\"There are {} data points in training data, each with {} attributes.\". format(num_records, num_attributes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd41045",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389b04ed",
   "metadata": {},
   "source": [
    "Before data cleaning, remove the known attributes that are not meaningful to our prediction model:\n",
    "  * Meaningless idendifier: listing_id \n",
    "  * Attributes in free text: title, description, features, accessories\n",
    "  * Attribute with the same value: eco_category, indicative_price\n",
    "  * Attribute unlikely to affect price: curb_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31a2cd8",
   "metadata": {},
   "source": [
    "We first drop columns with free text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c02d46d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 25000 data points in training data, each with 20 attributes.\n"
     ]
    }
   ],
   "source": [
    "columns_to_drop = [\n",
    "    'listing_id',          # Meaningless identifier\n",
    "    'description',\n",
    "    'features',\n",
    "    'accessories',\n",
    "    'eco_category',        # Attribute with the same value\n",
    "    'indicative_price',\n",
    "    'curb_weight',         # Attribute unlikely to affect price\n",
    "    'transmission',\n",
    "    'original_reg_date',\n",
    "    'lifespan',\n",
    "]\n",
    "\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "\n",
    "num_records, num_attributes = df.shape\n",
    "print(\"There are {} data points in training data, each with {} attributes.\". format(num_records, num_attributes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde0aaf8",
   "metadata": {},
   "source": [
    "### Handle Missing Values\n",
    "Firstly, for each of the columns with missing value, check the number of rows with NaN values.\n",
    "There are 3 scenarios:\n",
    "1. NaN value is the major (e.g. fuel_type has 19121 rows with NaN values), we remove the corresponding attritubes.\n",
    "2. NaN value is the minor. We can choose to fill or delete related data points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "834807be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data\n",
      "Column 'make' has 1316 rows with NaN values.\n",
      "Column 'fuel_type' has 19121 rows with NaN values.\n",
      "Column 'manufactured' has 7 rows with NaN values.\n",
      "Column 'power' has 2640 rows with NaN values.\n",
      "Column 'engine_cap' has 596 rows with NaN values.\n",
      "Column 'mileage' has 5304 rows with NaN values.\n",
      "Column 'no_of_owners' has 18 rows with NaN values.\n",
      "Column 'depreciation' has 507 rows with NaN values.\n",
      "Column 'road_tax' has 2632 rows with NaN values.\n",
      "Column 'dereg_value' has 220 rows with NaN values.\n",
      "Column 'omv' has 64 rows with NaN values.\n",
      "Column 'arf' has 174 rows with NaN values.\n",
      "Column 'opc_scheme' has 24838 rows with NaN values.\n"
     ]
    }
   ],
   "source": [
    "columns_to_check = [\n",
    "    'make',\n",
    "    'fuel_type',\n",
    "    'manufactured',\n",
    "    'power',\n",
    "    'engine_cap',\n",
    "    'mileage',\n",
    "    'no_of_owners',\n",
    "    'depreciation',\n",
    "    'road_tax',\n",
    "    'dereg_value',\n",
    "    'omv',\n",
    "    'arf',\n",
    "    'opc_scheme'\n",
    "]\n",
    "\n",
    "# Calculate the number of NaN values in each specified column\n",
    "nan_counts = df[columns_to_check].isna().sum()\n",
    "\n",
    "# Print the number of NaN values for each column\n",
    "print('Training data')\n",
    "for column, count in nan_counts.items():\n",
    "    print(f\"Column '{column}' has {count} rows with NaN values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2655f7",
   "metadata": {},
   "source": [
    "We first drop columns with TOO MANY NaN values and unlikely to help prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da6b4bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop_nan = [\n",
    "    'fuel_type',\n",
    "    'opc_scheme',\n",
    "]\n",
    "\n",
    "for col in columns_to_drop_nan:\n",
    "    if col in df.columns:\n",
    "        df = df.drop(columns=[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28900a18-8bf3-4516-b663-c00533fde54f",
   "metadata": {},
   "source": [
    "### Data transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09be75a",
   "metadata": {},
   "source": [
    "Transform date time attributes to numerical values\n",
    "This step is required to fill up the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce15c388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 25000 data points, each with 19 attributes.\n"
     ]
    }
   ],
   "source": [
    "from util.DataPreprocess import CalculateCarAge\n",
    "    \n",
    "\n",
    "df = CalculateCarAge(df)\n",
    "num_records, num_attributes = df.shape\n",
    "print(\"There are {} data points, each with {} attributes.\". format(num_records, num_attributes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279479f8",
   "metadata": {},
   "source": [
    "Then we fill column 'make' with title, since we notice that make is always covered by title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec0a24b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_set = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if pd.notna(row['make']) and row['make'] in row['title'].lower():\n",
    "        make_set.append(row['make'])\n",
    "        \n",
    "for index, row in df.iterrows():\n",
    "    if pd.isna(row['make']):\n",
    "        for make in make_set:\n",
    "            if make in row['title'].lower():\n",
    "                df.at[index, 'make'] = make.lower()\n",
    "                break\n",
    "                \n",
    "df = df.drop(columns=['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed39e693",
   "metadata": {},
   "source": [
    "Then we transform categorical data to numerical data. We first use LabelEncoder to do encoding and save the label mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "829f7d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = './data/test.csv'\n",
    "df_test = pd.read_csv(test_file)\n",
    "\n",
    "categorical_columns = [\n",
    "    'model',\n",
    "    'make',\n",
    "    'type_of_vehicle',\n",
    "]\n",
    "\n",
    "encode_dict = {}\n",
    "le = LabelEncoder()\n",
    "for column in categorical_columns:\n",
    "    df[column] = le.fit_transform(df[column])\n",
    "    encode_dict[column] = {str(label): int(index) for index, label in enumerate(le.classes_)}\n",
    "\n",
    "with open('./data/encode.json', 'w') as file:\n",
    "    json.dump(encode_dict, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a6b2b8",
   "metadata": {},
   "source": [
    "We then handle category column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9823aef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique categories: 15\n",
      "Unique categories: {'consignment car', 'parf car', 'low mileage car', 'electric cars', 'direct owner sale', 'hybrid cars', 'sta evaluated car', 'opc car', 'rare & exotic', 'imported used vehicle', 'coe car', 'sgcarmart warranty cars', 'almost new car', 'vintage cars', 'premium ad car'}\n",
      "There are 25000 data points, each with 32 attributes.\n",
      "Index(['make', 'model', 'manufactured', 'type_of_vehicle', 'power',\n",
      "       'engine_cap', 'no_of_owners', 'depreciation', 'coe', 'road_tax',\n",
      "       'dereg_value', 'mileage', 'omv', 'arf', 'price', 'reg_year', 'car_age',\n",
      "       'almost new car', 'coe car', 'consignment car', 'direct owner sale',\n",
      "       'electric cars', 'hybrid cars', 'imported used vehicle',\n",
      "       'low mileage car', 'opc car', 'parf car', 'premium ad car',\n",
      "       'rare & exotic', 'sgcarmart warranty cars', 'sta evaluated car',\n",
      "       'vintage cars'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from util.DataPreprocess import HandlingCategoryAttribute\n",
    "\n",
    "if 'category' in df.columns:\n",
    "    df = HandlingCategoryAttribute(df)\n",
    "    \n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22ec92b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data\n",
      "Column 'make' has 0 rows with NaN values.\n",
      "Column 'manufactured' has 7 rows with NaN values.\n",
      "Column 'engine_cap' has 596 rows with NaN values.\n",
      "Column 'mileage' has 5304 rows with NaN values.\n",
      "Column 'no_of_owners' has 18 rows with NaN values.\n",
      "Column 'depreciation' has 507 rows with NaN values.\n",
      "Column 'road_tax' has 2632 rows with NaN values.\n",
      "Column 'dereg_value' has 220 rows with NaN values.\n",
      "Column 'omv' has 64 rows with NaN values.\n",
      "Column 'arf' has 174 rows with NaN values.\n",
      "There are 24993 data points, each with 32 attributes.\n"
     ]
    }
   ],
   "source": [
    "columns_to_check = [\n",
    "    'make',\n",
    "    'manufactured',\n",
    "    'engine_cap',\n",
    "    'mileage',\n",
    "    'no_of_owners',\n",
    "    'depreciation',\n",
    "    'road_tax',\n",
    "    'dereg_value',\n",
    "    'omv',\n",
    "    'arf',\n",
    "]\n",
    "\n",
    "# Calculate the number of NaN values in each specified column\n",
    "nan_counts = df[columns_to_check].isna().sum()\n",
    "\n",
    "# Print the number of NaN values for each column\n",
    "print('Training data')\n",
    "for column, count in nan_counts.items():\n",
    "    print(f\"Column '{column}' has {count} rows with NaN values.\")\n",
    "\n",
    "df = df.dropna(subset=[\n",
    "    'manufactured',\n",
    "])\n",
    "\n",
    "num_records, num_attributes = df.shape\n",
    "print(\"There are {} data points, each with {} attributes.\". format(num_records, num_attributes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38e0486b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make                          0\n",
      "model                         0\n",
      "manufactured                  0\n",
      "type_of_vehicle               0\n",
      "power                      2639\n",
      "engine_cap                  593\n",
      "no_of_owners                 18\n",
      "depreciation                506\n",
      "coe                           0\n",
      "road_tax                   2627\n",
      "dereg_value                 219\n",
      "mileage                    5302\n",
      "omv                          64\n",
      "arf                         172\n",
      "price                         0\n",
      "reg_year                      0\n",
      "car_age                       0\n",
      "almost new car                0\n",
      "coe car                       0\n",
      "consignment car               0\n",
      "direct owner sale             0\n",
      "electric cars                 0\n",
      "hybrid cars                   0\n",
      "imported used vehicle         0\n",
      "low mileage car               0\n",
      "opc car                       0\n",
      "parf car                      0\n",
      "premium ad car                0\n",
      "rare & exotic                 0\n",
      "sgcarmart warranty cars       0\n",
      "sta evaluated car             0\n",
      "vintage cars                  0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "total_nulls = df.isnull().sum()\n",
    "print(total_nulls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea8a0de",
   "metadata": {},
   "source": [
    "### Then we try to fill up other missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfc6a526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['make', 'model', 'manufactured', 'type_of_vehicle', 'power',\n",
      "       'engine_cap', 'no_of_owners', 'depreciation', 'coe', 'road_tax',\n",
      "       'dereg_value', 'mileage', 'omv', 'arf', 'price', 'reg_year', 'car_age',\n",
      "       'almost new car', 'coe car', 'consignment car', 'direct owner sale',\n",
      "       'electric cars', 'hybrid cars', 'imported used vehicle',\n",
      "       'low mileage car', 'opc car', 'parf car', 'premium ad car',\n",
      "       'rare & exotic', 'sgcarmart warranty cars', 'sta evaluated car',\n",
      "       'vintage cars'],\n",
      "      dtype='object')\n",
      "NaN values after handling:  0\n",
      "make                       0\n",
      "model                      0\n",
      "manufactured               0\n",
      "type_of_vehicle            0\n",
      "power                      0\n",
      "engine_cap                 0\n",
      "no_of_owners               0\n",
      "depreciation               0\n",
      "coe                        0\n",
      "road_tax                   0\n",
      "dereg_value                0\n",
      "mileage                    0\n",
      "omv                        0\n",
      "arf                        0\n",
      "price                      0\n",
      "reg_year                   0\n",
      "car_age                    0\n",
      "almost new car             0\n",
      "coe car                    0\n",
      "consignment car            0\n",
      "direct owner sale          0\n",
      "electric cars              0\n",
      "hybrid cars                0\n",
      "imported used vehicle      0\n",
      "low mileage car            0\n",
      "opc car                    0\n",
      "parf car                   0\n",
      "premium ad car             0\n",
      "rare & exotic              0\n",
      "sgcarmart warranty cars    0\n",
      "sta evaluated car          0\n",
      "vintage cars               0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from util.DataPreprocess import HandlingMissingValue\n",
    "from util.DataPreprocess import HandlingMissingValueWithImpute\n",
    "\n",
    "columns = df.columns\n",
    "\n",
    "print(df.columns)\n",
    "\n",
    "df = HandlingMissingValue(df)\n",
    "df = HandlingMissingValueWithImpute(df, columns)\n",
    "total_nulls = df.isnull().sum()\n",
    "print(total_nulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8c1cf5f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>make</th>\n",
       "      <th>model</th>\n",
       "      <th>manufactured</th>\n",
       "      <th>type_of_vehicle</th>\n",
       "      <th>power</th>\n",
       "      <th>engine_cap</th>\n",
       "      <th>no_of_owners</th>\n",
       "      <th>depreciation</th>\n",
       "      <th>coe</th>\n",
       "      <th>road_tax</th>\n",
       "      <th>...</th>\n",
       "      <th>hybrid cars</th>\n",
       "      <th>imported used vehicle</th>\n",
       "      <th>low mileage car</th>\n",
       "      <th>opc car</th>\n",
       "      <th>parf car</th>\n",
       "      <th>premium ad car</th>\n",
       "      <th>rare &amp; exotic</th>\n",
       "      <th>sgcarmart warranty cars</th>\n",
       "      <th>sta evaluated car</th>\n",
       "      <th>vintage cars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43.0</td>\n",
       "      <td>595.0</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>2995.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>34270.0</td>\n",
       "      <td>48011.0</td>\n",
       "      <td>2380.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21170.0</td>\n",
       "      <td>47002.0</td>\n",
       "      <td>1202.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29.0</td>\n",
       "      <td>546.0</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2354.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12520.0</td>\n",
       "      <td>50355.0</td>\n",
       "      <td>2442.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1598.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10140.0</td>\n",
       "      <td>27571.0</td>\n",
       "      <td>1113.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>2995.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13690.0</td>\n",
       "      <td>48479.0</td>\n",
       "      <td>3570.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   make  model  manufactured  type_of_vehicle  power  engine_cap  \\\n",
       "0  43.0  595.0        2018.0              8.0  280.0      2995.0   \n",
       "1  51.0  192.0        2017.0              2.0  135.0      1991.0   \n",
       "2  29.0  546.0        2007.0              4.0  118.0      2354.0   \n",
       "3  88.0  156.0        2008.0              3.0   80.0      1598.0   \n",
       "4  44.0  398.0        2006.0              2.0  183.0      2995.0   \n",
       "\n",
       "   no_of_owners  depreciation      coe  road_tax  ...  hybrid cars  \\\n",
       "0           2.0       34270.0  48011.0    2380.0  ...          0.0   \n",
       "1           2.0       21170.0  47002.0    1202.0  ...          0.0   \n",
       "2           3.0       12520.0  50355.0    2442.0  ...          0.0   \n",
       "3           3.0       10140.0  27571.0    1113.0  ...          0.0   \n",
       "4           6.0       13690.0  48479.0    3570.0  ...          0.0   \n",
       "\n",
       "   imported used vehicle  low mileage car  opc car  parf car  premium ad car  \\\n",
       "0                    0.0              0.0      0.0       1.0             0.0   \n",
       "1                    0.0              0.0      0.0       1.0             1.0   \n",
       "2                    0.0              1.0      0.0       0.0             1.0   \n",
       "3                    0.0              0.0      0.0       0.0             1.0   \n",
       "4                    0.0              0.0      0.0       0.0             1.0   \n",
       "\n",
       "   rare & exotic  sgcarmart warranty cars  sta evaluated car  vintage cars  \n",
       "0            0.0                      0.0                0.0           0.0  \n",
       "1            0.0                      0.0                0.0           0.0  \n",
       "2            0.0                      0.0                0.0           0.0  \n",
       "3            0.0                      0.0                0.0           0.0  \n",
       "4            0.0                      0.0                0.0           0.0  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eafaf03",
   "metadata": {},
   "source": [
    "### Remove Exact Duplicates\n",
    "We remove duplicated data points here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8caa0e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 24415 data points in training data, each with 32 attributes.\n"
     ]
    }
   ],
   "source": [
    "df = df.drop_duplicates()\n",
    "\n",
    "num_records, num_attributes = df.shape\n",
    "print(\"There are {} data points in training data, each with {} attributes.\". format(num_records, num_attributes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a55c35-f9f7-42ed-a7aa-1e3edfffa7d4",
   "metadata": {},
   "source": [
    "### Data Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c95ae45d-0846-4e1b-9f4d-2608349504b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.DataPreprocess import DataCalculation\n",
    "\n",
    "df = DataCalculation(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11789885",
   "metadata": {},
   "source": [
    "### Save the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10b15971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing file './data/train_preprocessed_impute.csv' has been deleted.\n",
      "DataFrame has been saved to './data/train_preprocessed_impute.csv'.\n"
     ]
    }
   ],
   "source": [
    "file_name = './data/train_preprocessed_impute.csv'\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(file_name):\n",
    "    # Delete the file\n",
    "    os.remove(file_name)\n",
    "    print(f\"Existing file '{file_name}' has been deleted.\")\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "df.to_csv(file_name, index=False)\n",
    "print(f\"DataFrame has been saved to '{file_name}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a1b610-13c2-4709-9ad9-cb3482c3d209",
   "metadata": {},
   "source": [
    "### Load preprocessed training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6e80ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['make', 'model', 'manufactured', 'type_of_vehicle', 'power',\n",
      "       'engine_cap', 'no_of_owners', 'depreciation', 'coe', 'road_tax',\n",
      "       'dereg_value', 'mileage', 'omv', 'arf', 'price', 'reg_year', 'car_age',\n",
      "       'almost new car', 'coe car', 'consignment car', 'direct owner sale',\n",
      "       'electric cars', 'hybrid cars', 'imported used vehicle',\n",
      "       'low mileage car', 'opc car', 'parf car', 'premium ad car',\n",
      "       'rare & exotic', 'sgcarmart warranty cars', 'sta evaluated car',\n",
      "       'vintage cars', 'omv_arf_ratio', 'dereg_coe_ratio'],\n",
      "      dtype='object')\n",
      "There are 24415 data points in training data, each with 34 attributes.\n"
     ]
    }
   ],
   "source": [
    "# Load file into pandas dataframe, we saved our preprocessed file at path 'output_file'\n",
    "training_file = './data/train_preprocessed_impute.csv'\n",
    "df = pd.read_csv(training_file)\n",
    "\n",
    "columns_to_keep = [col for col in df.columns if col != 'price']\n",
    "\n",
    "print(df.columns)\n",
    "num_records, num_attributes = df.shape\n",
    "print(\"There are {} data points in training data, each with {} attributes.\". format(num_records, num_attributes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77da5708",
   "metadata": {},
   "source": [
    "### Data Augmentation, copy rows with less than 5 samples by group. The augmentated data is used only for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1aa49ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 40693 data points after augmentation, each with 34 attributes.\n"
     ]
    }
   ],
   "source": [
    "from util.DataPreprocess import DataAugmentation\n",
    "\n",
    "df_aug = DataAugmentation(df)\n",
    "\n",
    "num_records, num_attributes = df_aug.shape\n",
    "print(\"There are {} data points after augmentation, each with {} attributes.\". format(num_records, num_attributes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca582db1",
   "metadata": {},
   "source": [
    "### Save the augmentation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4d43381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing file './data/train_preprocessed_augmentation.csv' has been deleted.\n",
      "DataFrame has been saved to './data/train_preprocessed_augmentation.csv'.\n"
     ]
    }
   ],
   "source": [
    "file_name = './data/train_preprocessed_augmentation.csv'\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(file_name):\n",
    "    # Delete the file\n",
    "    os.remove(file_name)\n",
    "    print(f\"Existing file '{file_name}' has been deleted.\")\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "df_aug.to_csv(file_name, index=False)\n",
    "print(f\"DataFrame has been saved to '{file_name}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4c9029",
   "metadata": {},
   "source": [
    "## Preprocess test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81365da5-d652-4c94-9cb9-0bdbc430dc9e",
   "metadata": {},
   "source": [
    "Load test data and preprocess. We also need training data to help here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51b11652-d408-4b2d-b7a0-a39b1dafd5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = './data/train_preprocessed_impute.csv'\n",
    "df = pd.read_csv(train_file)\n",
    "\n",
    "test_file = './data/test.csv'\n",
    "df_test = pd.read_csv(test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76970dd9",
   "metadata": {},
   "source": [
    "We first drop columns with free text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9155d013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10000 data points in training data, each with 19 attributes.\n"
     ]
    }
   ],
   "source": [
    "columns_to_drop = [\n",
    "    'listing_id',          # Meaningless identifier\n",
    "    'description',\n",
    "    'features',\n",
    "    'accessories',\n",
    "    'eco_category',        # Attribute with the same value\n",
    "    'indicative_price',\n",
    "    'curb_weight',         # Attribute unlikely to affect price\n",
    "    'transmission',\n",
    "    'original_reg_date',\n",
    "    'lifespan',\n",
    "]\n",
    "\n",
    "df_test = df_test.drop(columns=columns_to_drop)\n",
    "\n",
    "num_records, num_attributes = df_test.shape\n",
    "print(\"There are {} data points in training data, each with {} attributes.\". format(num_records, num_attributes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f062d2a",
   "metadata": {},
   "source": [
    "### Calculate car age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e6ecdb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10000 data points, each with 20 attributes.\n"
     ]
    }
   ],
   "source": [
    "from util.DataPreprocess import CalculateCarAge\n",
    "df_test = CalculateCarAge(df_test)\n",
    "\n",
    "num_records, num_attributes = df_test.shape\n",
    "print(\"There are {} data points, each with {} attributes.\". format(num_records, num_attributes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35084373",
   "metadata": {},
   "source": [
    "### Convert category data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "434bdf6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique categories: 15\n",
      "Unique categories: {'consignment car', 'parf car', 'low mileage car', 'electric cars', 'direct owner sale', 'hybrid cars', 'sta evaluated car', 'opc car', 'rare & exotic', 'imported used vehicle', 'coe car', 'sgcarmart warranty cars', 'almost new car', 'vintage cars', 'premium ad car'}\n",
      "There are 10000 data points, each with 34 attributes.\n"
     ]
    }
   ],
   "source": [
    "from util.DataPreprocess import HandlingCategoryAttribute\n",
    "\n",
    "if 'category' in df_test.columns:\n",
    "    df_test = HandlingCategoryAttribute(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bebacb",
   "metadata": {},
   "source": [
    "### Handle missing values on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1842516",
   "metadata": {},
   "source": [
    "We first drop columns with TOO MANY NaN values and unlikely to help prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6bd49a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop_nan = [\n",
    "    'fuel_type',\n",
    "    'opc_scheme',\n",
    "]\n",
    "\n",
    "for col in columns_to_drop_nan:\n",
    "    if col in df_test.columns:\n",
    "        df_test = df_test.drop(columns=[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afbf62d",
   "metadata": {},
   "source": [
    "We also fill column 'make' with 'title' here as we did before on train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "35dde36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_set = []\n",
    "\n",
    "for index, row in df_test.iterrows():\n",
    "    if pd.notna(row['make']) and row['make'] in row['title'].lower():\n",
    "        make_set.append(row['make'])\n",
    "        \n",
    "for index, row in df_test.iterrows():\n",
    "    if pd.isna(row['make']):\n",
    "        for make in make_set:\n",
    "            if make in row['title'].lower():\n",
    "                df_test.at[index, 'make'] = make.lower()\n",
    "                break\n",
    "                \n",
    "df_test = df_test.drop(columns=['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2786c79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data\n",
      "make                          0\n",
      "model                         0\n",
      "manufactured                  3\n",
      "type_of_vehicle               0\n",
      "power                      1086\n",
      "engine_cap                  235\n",
      "no_of_owners                  8\n",
      "depreciation                201\n",
      "coe                           0\n",
      "road_tax                   1082\n",
      "dereg_value                  83\n",
      "mileage                    2166\n",
      "omv                          29\n",
      "arf                          65\n",
      "reg_year                      0\n",
      "car_age                       0\n",
      "almost new car                0\n",
      "coe car                       0\n",
      "consignment car               0\n",
      "direct owner sale             0\n",
      "electric cars                 0\n",
      "hybrid cars                   0\n",
      "imported used vehicle         0\n",
      "low mileage car               0\n",
      "opc car                       0\n",
      "parf car                      0\n",
      "premium ad car                0\n",
      "rare & exotic                 0\n",
      "sgcarmart warranty cars       0\n",
      "sta evaluated car             0\n",
      "vintage cars                  0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "columns_to_check = [\n",
    "    'make',\n",
    "    'model',\n",
    "    'manufactured',\n",
    "    'engine_cap',\n",
    "    'no_of_owners',\n",
    "    'depreciation',\n",
    "    'road_tax',\n",
    "    'mileage',\n",
    "    'dereg_value'\n",
    "]\n",
    "\n",
    "# Calculate the number of NaN values in each specified column\n",
    "nan_counts = df_test[columns_to_check].isna().sum()\n",
    "\n",
    "# Print the number of NaN values for each column\n",
    "print('Test data')\n",
    "total_nulls = df_test.isnull().sum()\n",
    "print(total_nulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d2768fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.DataPreprocess import HandlingCategoryAttribute\n",
    "\n",
    "if 'category' in df_test.columns:\n",
    "    df_test = HandlingCategoryAttribute(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6508fc75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['make', 'model', 'manufactured', 'type_of_vehicle', 'power',\n",
      "       'engine_cap', 'no_of_owners', 'depreciation', 'coe', 'road_tax',\n",
      "       'dereg_value', 'mileage', 'omv', 'arf', 'price', 'reg_year', 'car_age',\n",
      "       'almost new car', 'coe car', 'consignment car', 'direct owner sale',\n",
      "       'electric cars', 'hybrid cars', 'imported used vehicle',\n",
      "       'low mileage car', 'opc car', 'parf car', 'premium ad car',\n",
      "       'rare & exotic', 'sgcarmart warranty cars', 'sta evaluated car',\n",
      "       'vintage cars', 'omv_arf_ratio', 'dereg_coe_ratio'],\n",
      "      dtype='object')\n",
      "NaN values after handling:  3872\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'honda'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m      7\u001b[0m df_test \u001b[38;5;241m=\u001b[39m HandlingMissingValueTest(df_test)\n\u001b[0;32m----> 8\u001b[0m df_test \u001b[38;5;241m=\u001b[39m \u001b[43mHandlingMissingValueWithImputeReference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/NUS/Sem1_AY2024-2025/CS5228/Projects/CS5228_Project/util/DataPreprocess.py:314\u001b[0m, in \u001b[0;36mHandlingMissingValueWithImputeReference\u001b[0;34m(df_target, df_reference, columns)\u001b[0m\n\u001b[1;32m    312\u001b[0m combined_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df_target[columns], df_reference[columns]], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    313\u001b[0m imputer \u001b[38;5;241m=\u001b[39m IterativeImputer(estimator\u001b[38;5;241m=\u001b[39mBayesianRidge(), max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 314\u001b[0m imputed_array \u001b[38;5;241m=\u001b[39m \u001b[43mimputer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m imputed_target \u001b[38;5;241m=\u001b[39m imputed_array[:\u001b[38;5;28mlen\u001b[39m(df_target)]\n\u001b[1;32m    317\u001b[0m df_imputed \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(imputed_target, columns\u001b[38;5;241m=\u001b[39mcolumns, index\u001b[38;5;241m=\u001b[39mdf_target\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/sklearn/utils/_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    301\u001b[0m         )\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/sklearn/impute/_iterative.py:720\u001b[0m, in \u001b[0;36mIterativeImputer.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimputation_sequence_ \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitial_imputer_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 720\u001b[0m X, Xt, mask_missing_values, complete_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initial_imputation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_fit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    722\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_fit_indicator(complete_mask)\n\u001b[1;32m    725\u001b[0m X_indicator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_transform_indicator(complete_mask)\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/sklearn/impute/_iterative.py:613\u001b[0m, in \u001b[0;36mIterativeImputer._initial_imputation\u001b[0;34m(self, X, in_fit)\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    611\u001b[0m     force_all_finite \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 613\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mF\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_fit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m _check_inputs_dtype(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing_values)\n\u001b[1;32m    622\u001b[0m X_missing_mask \u001b[38;5;241m=\u001b[39m _get_mask(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing_values)\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/sklearn/base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[1;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:997\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    995\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    996\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 997\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    998\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1000\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[1;32m   1001\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/sklearn/utils/_array_api.py:521\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[1;32m    519\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 521\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/pandas/core/generic.py:2084\u001b[0m, in \u001b[0;36mNDFrame.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   2082\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype: npt\u001b[38;5;241m.\u001b[39mDTypeLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m   2083\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m-> 2084\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2085\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2086\u001b[0m         astype_is_view(values\u001b[38;5;241m.\u001b[39mdtype, arr\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m   2087\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write()\n\u001b[1;32m   2088\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mis_single_block\n\u001b[1;32m   2089\u001b[0m     ):\n\u001b[1;32m   2090\u001b[0m         \u001b[38;5;66;03m# Check if both conversions can be done without a copy\u001b[39;00m\n\u001b[1;32m   2091\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m astype_is_view(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtypes\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m], values\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mand\u001b[39;00m astype_is_view(\n\u001b[1;32m   2092\u001b[0m             values\u001b[38;5;241m.\u001b[39mdtype, arr\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m   2093\u001b[0m         ):\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'honda'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f94ae90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_test = HandlingMissingValueWithReference(df, df_test)\n",
    "total_nulls = df_test.isnull().sum()\n",
    "print(total_nulls)\n",
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c331190",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from util.DataPreprocess import DataCalculation\n",
    "\n",
    "df_test = DataCalculation(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e85bd5-970c-425c-a0f0-dce631a8d3a7",
   "metadata": {},
   "source": [
    "### Encode attributes on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e21a079-8608-48b9-bd9e-389eb321d103",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_records, num_attributes = df_test.shape\n",
    "print(\"There are {} data points, each with {} attributes.\". format(num_records, num_attributes))\n",
    "\n",
    "categorical_columns = [\n",
    "    'make',\n",
    "    'model',\n",
    "    'type_of_vehicle'\n",
    "]\n",
    "\n",
    "with open('./data/encode.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "new_encodings = {}\n",
    "\n",
    "for col, cate_dict in data.items():\n",
    "    if col in df_test.columns and col in categorical_columns:\n",
    "        original_values = df_test[col].copy()\n",
    "        \n",
    "        df_test[col] = df_test[col].map(cate_dict)\n",
    "        \n",
    "        missing = df_test[col][df_test[col].isna()]\n",
    "        if not missing.empty:\n",
    "            original_missing_values = original_values[missing.index]\n",
    "            unique_missing_values = original_missing_values.unique()\n",
    "            new_encoding_dict = {val: idx for idx, val in enumerate(unique_missing_values, start=max(cate_dict.values()) + 1)}\n",
    "            new_encodings[col] = new_encoding_dict\n",
    "            \n",
    "            df_test[col].fillna(original_missing_values.map(new_encoding_dict), inplace=True)\n",
    "\n",
    "if new_encodings:\n",
    "    print(\"New encodings created for missing values:\")\n",
    "    for col, encodings in new_encodings.items():\n",
    "        print(f\"Column: {col}, New encodings: {encodings}\")\n",
    "else:\n",
    "    print(\"No new encodings were necessary.\")\n",
    "\n",
    "print(df_test.head())\n",
    "\n",
    "num_records, num_attributes = df_test.shape\n",
    "print(f\"There are {num_records} data points in test data, each with {num_attributes} attributes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6223e833",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from util.DataPreprocess import HandlingMissingValueWithImputeReference\n",
    "from util.DataPreprocess import HandlingMissingValueTest\n",
    "\n",
    "columns = df_test.columns\n",
    "print(df.columns)\n",
    "\n",
    "df_test = HandlingMissingValueTest(df_test)\n",
    "df_test = HandlingMissingValueWithImputeReference(df_test, df, columns)\n",
    "\n",
    "total_nulls = df_test.isnull().sum()\n",
    "print(total_nulls)\n",
    "\n",
    "df_test.to_csv('./data/test_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fcf486",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f613860d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cba56ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
